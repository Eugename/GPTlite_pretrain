{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "defadcf2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2fbd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002e4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm import tqdm\n",
    "#tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73066179",
   "metadata": {},
   "source": [
    "Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getenv(\"PATH\")\n",
    "DATAPATH = os.getenv(\"DATAPATH\")\n",
    "PREPARED_DATA_DIR = os.getenv(\"PREPARED_DATA_DIR\")\n",
    "CACHE_DIR = os.getenv(\"CACHE_DIR\")\n",
    "#TOK_NAME = \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\"\n",
    "TOK_NAME = os.getenv(\"TOK_NAME\")\n",
    "PARQUET_DATA_DIR = os.getenv(\"PARQUET_DATA_DIR\")\n",
    "CSV1 = 'G:\\\\My_files\\\\Programming\\\\My_projects\\\\LLM\\\\GPT-like_trained\\\\Data\\\\Processed\\\\CSV\\\\1_Tokenized_2049_padded.csv'\n",
    "CSV2 = 'G:\\\\My_files\\\\Programming\\\\My_projects\\\\LLM\\\\GPT-like_trained\\\\Data\\\\Processed\\\\CSV\\\\2_Tokenized_2049_padded.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973bee7d",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ce1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "    'vocab_size': 50257, # in 151670 (if you use tokenizer.vocab_size then you get partial vocab_size without added tokens)\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 768, #768\n",
    "    'n_heads': 2,#12,\n",
    "    'n_layers': 2,#12,\n",
    "    'drop_rate': 0.05, # 0l1\n",
    "    'qkv_bias': False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eddb707",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if (torch.cuda.is_available()) else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5a8a01",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6926e17f",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a67da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = transformers.AutoTokenizer.from_pretrained(TOK_NAME, cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16aa42e",
   "metadata": {},
   "source": [
    "Check tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eebc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.get_added_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23012250",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8492d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If tokenizer dont have pad_token\n",
    "tok.pad_token = tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok('Привет, как дела mhjm', return_tensors='pt', padding='max_length', max_length=2048)['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b1a01b",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af45dde7",
   "metadata": {},
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a2fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATAPATH, encoding='utf8', mode='r') as file:\n",
    "    d = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb0782",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a0bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "1423181938//131072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9f11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chunks=25\n",
    "stride = len(d)//num_chunks\n",
    "\n",
    "for i, chunk_idx in tqdm(enumerate(range(0, len(d), stride))):\n",
    "    with open(os.path.join(PREPARED_DATA_DIR, f'chunk_{i}.txt'), mode='w') as file:\n",
    "        file.write(d[chunk_idx:chunk_idx+stride])\n",
    "    print(i, chunk_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d75d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec81c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_chunks=25\n",
    "stride = 131072#len(d)//num_chunks\n",
    "\n",
    "data_parquet = pd.DataFrame([], columns=['Sample', 'Chunk'])\n",
    "for i, chunk_idx in tqdm(enumerate(range(0, len(d), stride))):\n",
    "    data_parquet.loc[len(data_parquet)] = ['sdgsgsg', 0]\n",
    "    #with open(os.path.join(PREPARED_DATA_DIR, f'chunk_{i}.txt'), mode='w') as file:\n",
    "    #    file.write(d[chunk_idx:chunk_idx+stride])\n",
    "    print(i, chunk_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad334bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = pd.DataFrame([], columns=['Sample', 'Chunk'])\n",
    "for i, filename in tqdm(enumerate(os.listdir(PREPARED_DATA_DIR)), total=len(os.listdir(PREPARED_DATA_DIR))):\n",
    "    with open(os.path.join(PREPARED_DATA_DIR, filename), encoding='utf8', mode='r') as file:\n",
    "        current_file = file.read()\n",
    "        stride = 2048*3\n",
    "        mas = ''\n",
    "        for article in current_file.split('/n'):\n",
    "            for sentence in article.split('.'):\n",
    "                if (len(mas)+len(sentence) < stride):\n",
    "                    mas += sentence\n",
    "                else:\n",
    "                    data_parquet.loc[len(data_parquet)] = [mas, i]\n",
    "                    mas = ''\n",
    "            \n",
    "        # for chunk_idx in tqdm(range(0, len(current_file), stride)):\n",
    "        #     current_chunk = current_file[chunk_idx:chunk_idx+stride]\n",
    "        #     data_parquet.loc[len(data_parquet)] = ['sdgsgsg', 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add94d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "2048*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfa021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet.to_parquet(PARQUET_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494eaafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d[200:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c2afb6",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ec67e",
   "metadata": {},
   "source": [
    "Небольшой анализ длины предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PREPARED_DATA_DIR, os.listdir(PREPARED_DATA_DIR)[0]), encoding='utf8', mode='r') as file:\n",
    "    d = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce63495",
   "metadata": {},
   "outputs": [],
   "source": [
    "splt = d.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35715797",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(elem) for elem in splt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5e3858",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce341a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lens, bins=20, range=(0, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10caf1b",
   "metadata": {},
   "source": [
    "Если взять длину абзацев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b6afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "splt = d.split('/n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "splt[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dacdcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(elem) for elem in splt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38655215",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd28db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lens, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba88284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea1db59e",
   "metadata": {},
   "source": [
    "Class for dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0765f88",
   "metadata": {},
   "source": [
    "## Old versions of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d8c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetV1(Dataset):\n",
    "    def __init__(self, txt: str, tokenizer: object, max_length: int, stride: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ce3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetV2(Dataset):\n",
    "    def __init__(self, dataframe: str, tokenizer: object, max_length: int, stride: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        for i, curr_chunk in dataframe.iterrows():\n",
    "            token_ids = tokenizer.encode(curr_chunk['Sample'])\n",
    "            for i in range(0, len(token_ids) - max_length, stride):\n",
    "                input_chunk = token_ids[i:i + max_length]\n",
    "                target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "                self.input_ids.append(torch.tensor(input_chunk))\n",
    "                self.target_ids.append(torch.tensor(target_chunk))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = CustomDatasetV2(dataframe=data_parquet.iloc[:100], tokenizer=tok, max_length=1024, stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e848aa",
   "metadata": {},
   "source": [
    "## Let's tokenize separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a31d2",
   "metadata": {},
   "source": [
    "Let's tokenize separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ac3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = pd.read_parquet(PARQUET_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fbac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet.shape[0] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f38348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef46505",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data_parquet['Sample'].iloc[:100000].progress_apply(lambda curr_chunk: tok(curr_chunk, padding='max_length', max_length=2048+1)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1698602",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = data_parquet['Sample'].iloc[100000:].progress_apply(lambda curr_chunk: tok(curr_chunk, padding='max_length', max_length=2048+1)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa0bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.DataFrame(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b90028",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = pd.DataFrame(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d372b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1['Chunk'] = data_parquet['Chunk'].iloc[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1da49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2['Chunk'] = data_parquet['Chunk'].iloc[100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13db886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1['Sample'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db3cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aaa302",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.to_csv(CSV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c4e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea19cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0afa830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tokens(curr_chunk):\n",
    "    return tok(curr_chunk, padding='max_length', max_length=1024+1)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f1b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_parquet['Sample'].parallel_apply(tokenize_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a8deee",
   "metadata": {},
   "source": [
    "## Version of dataset with tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetV3(Dataset):\n",
    "    def __init__(self, dataframe: str, tokenizer: object, max_length: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        for i, curr_chunk in tqdm(dataframe.iterrows(), total=dataframe.shape[0]):\n",
    "            token_ids = tokenizer(curr_chunk['Sample'], return_tensors='pt', padding='max_length', max_length=max_length+1)['input_ids']\n",
    "            input_chunk = token_ids[:,:max_length].view(-1)\n",
    "            target_chunk = token_ids[:,1:max_length+1].view(-1)\n",
    "            #print(input_chunk.size(), target_chunk.size(),)\n",
    "            self.input_ids.append(input_chunk)\n",
    "            self.target_ids.append(target_chunk)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7005f4ca",
   "metadata": {},
   "source": [
    "## Version of dataset without tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168caeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetV4(Dataset):\n",
    "    def __init__(self, dataframe: str, max_length: int):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        for i, curr_chunk in tqdm(dataframe.iterrows(), total=dataframe.shape[0]):\n",
    "            token_ids = torch.tensor(json.loads(curr_chunk['Sample']))\n",
    "            input_chunk = token_ids[:max_length].view(-1)\n",
    "            target_chunk = token_ids[1:max_length+1].view(-1)\n",
    "            self.input_ids.append(input_chunk)\n",
    "            self.target_ids.append(target_chunk)\n",
    "\n",
    "    # Cannot use vector arifmetic, because pandas cannor recognize torch.tensor type\n",
    "    # def to_torch(x):\n",
    "    #     token_ids = torch.tensor(json.loads(x))\n",
    "    #     return token_ids[:2048].view(-1)\n",
    "    #data['Sample'].iloc[:100].apply(to_torch)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352244c0",
   "metadata": {},
   "source": [
    "# Load data with tokenizer and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c08da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = pd.read_parquet(PARQUET_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79756d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdba844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cd = CustomDatasetV3(dataframe=data_parquet.iloc[:100000], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])\n",
    "#train_cd = CustomDatasetV3(dataframe=data_parquet.iloc[:100], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82364c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cd = CustomDatasetV3(dataframe=data_parquet.iloc[-10000:], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])\n",
    "#val_cd = CustomDatasetV3(dataframe=data_parquet.iloc[-100:], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2a302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(dataset=train_cd, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_data = DataLoader(dataset=val_cd, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ac2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40801fce",
   "metadata": {},
   "source": [
    "# Load data without tokenizer and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d70448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(CSV1, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a2e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cd = CustomDatasetV4(dataframe=data.iloc[:90000], max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])\n",
    "#train_cd = CustomDatasetV3(dataframe=data_parquet.iloc[:100], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760dacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cd = CustomDatasetV4(dataframe=data.iloc[-10000:], max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])\n",
    "#val_cd = CustomDatasetV3(dataframe=data_parquet.iloc[-100:], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(dataset=train_cd, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_data = DataLoader(dataset=val_cd, batch_size=8, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae6a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_data))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73468d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.batch_decode(next(iter(train_data))[0]) # Work properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32f1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
