{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5efd75dc",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36b4b631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5368f38f",
   "metadata": {},
   "source": [
    "Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a704fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getenv(\"PATH\")\n",
    "DATAPATH = os.getenv(\"DATAPATH\")\n",
    "PREPARED_DATA_DIR = os.getenv(\"PREPARED_DATA_DIR\")\n",
    "CACHE_DIR = os.getenv(\"CACHE_DIR\")\n",
    "#TOK_NAME = \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\"\n",
    "TOK_NAME = os.getenv(\"TOK_NAME\")\n",
    "PARQUET_DATA_DIR = os.getenv(\"PARQUET_DATA_DIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540273ba",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f39ff365",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "    'vocab_size': 50257, # in 151670 (if you use tokenizer.vocab_size then you get partial vocab_size without added tokens)\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 768, #768\n",
    "    'n_heads': 2,#12,\n",
    "    'n_layers': 2,#12,\n",
    "    'drop_rate': 0.05, # 0l1\n",
    "    'qkv_bias': False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c41445b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if (torch.cuda.is_available()) else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fabcf41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(424242)\n",
    "batch_size = 8\n",
    "input_vector = torch.randint(0, 50000, size=(batch_size, 1024), device=device)\n",
    "input_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2883b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f227ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, x_input):\n",
    "    if x_input.grad is not None:\n",
    "        x_input.grad.zero_()\n",
    "    _ = model(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f381281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(model, x_input, loss):\n",
    "    if x_input.grad is not None:\n",
    "        x_input.grad.zero_()\n",
    "\n",
    "    output = model(x_input)\n",
    "    loss = loss(output.flatten(0, 1), x_input.flatten()) # Exasmple ^_^: loss = nn.functional.cross_entropy(logits.flatten(0, 1), y.flatten())\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef236cf5",
   "metadata": {},
   "source": [
    "## Неоптимизированная архитектура"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bffce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0)\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.size()\n",
    "        keys = self.W_key(x) # b, num_tokens, self.d_out\n",
    "        queries = self.W_query(x) # b, num_tokens, self.d_out\n",
    "        values = self.W_value(x) # b, num_tokens, self.d_out\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "        queries = queries.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "        values = values.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "\n",
    "        att_scores = queries @ keys.transpose(2, 3) # shapes = (num_tokens, self.head_dim) @ (self.head_dim, num_tokens) -> (num_tokens, num_tokens)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        att_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        att_weights = torch.softmax(att_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        att_weights = self.dropout(att_weights)\n",
    "\n",
    "        context_vec = (att_weights @ values).transpose(1, 2) # (num_tokens, num_tokens) @ (num_tokens, self.head_dim) -> (num_tokens, self.head_dim) -> transpose(1,2) of (b, self.num_heads, num_tokens, self.head_dim) ->\n",
    "        # -> (b, num_tokens, self.num_heads, self.head_dim) as view in previous code after inference of Linear layers\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c15d187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f188df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b5740bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim'])\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca22034",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_in=cfg['emb_dim'], \n",
    "                                       d_out=cfg['emb_dim'], \n",
    "                                       context_length=cfg['context_length'], \n",
    "                                       dropout=cfg['drop_rate'], \n",
    "                                       num_heads=cfg['n_heads'], \n",
    "                                       qkv_bias=cfg['qkv_bias'])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_resid = nn.Dropout(cfg['drop_rate'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = x + self.drop_resid(self.attn(self.norm1(x)))\n",
    "        #x = x + self.drop_resid(self.ff(self.norm2(x)))\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53380a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "        self.final_norm = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.size()\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d537fd0",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8992f1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.05, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_GPT = GPTModel(GPT_CONFIG)\n",
    "vanilla_GPT.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1be70d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 ms ± 4.49 ms per loop (mean ± std. dev. of 25 runs, 25 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 25 -r 25\n",
    "forward(vanilla_GPT, input_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a57ce",
   "metadata": {},
   "source": [
    "## Replace with Pytohch built-in functions, but with current handwritten self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21d37edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0)\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.size()\n",
    "        keys = self.W_key(x) # b, num_tokens, self.d_out\n",
    "        queries = self.W_query(x) # b, num_tokens, self.d_out\n",
    "        values = self.W_value(x) # b, num_tokens, self.d_out\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "        queries = queries.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "        values = values.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "\n",
    "        att_scores = queries @ keys.transpose(2, 3) # shapes = (num_tokens, self.head_dim) @ (self.head_dim, num_tokens) -> (num_tokens, num_tokens)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        att_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        att_weights = torch.softmax(att_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        att_weights = self.dropout(att_weights)\n",
    "\n",
    "        context_vec = (att_weights @ values).transpose(1, 2) # (num_tokens, num_tokens) @ (num_tokens, self.head_dim) -> (num_tokens, self.head_dim) -> transpose(1,2) of (b, self.num_heads, num_tokens, self.head_dim) ->\n",
    "        # -> (b, num_tokens, self.num_heads, self.head_dim) as view in previous code after inference of Linear layers\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af3bb6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim']),\n",
    "            nn.GELU(), #GELU(),\n",
    "            nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim'])\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5f0caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_in=cfg['emb_dim'], \n",
    "                                       d_out=cfg['emb_dim'], \n",
    "                                       context_length=cfg['context_length'], \n",
    "                                       dropout=cfg['drop_rate'], \n",
    "                                       num_heads=cfg['n_heads'], \n",
    "                                       qkv_bias=cfg['qkv_bias'])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = nn.LayerNorm(cfg['emb_dim']) #LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = nn.LayerNorm(cfg['emb_dim']) #LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_resid = nn.Dropout(cfg['drop_rate'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = x + self.drop_resid(self.attn(self.norm1(x)))\n",
    "        #x = x + self.drop_resid(self.ff(self.norm2(x)))\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "373d3e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModelPT(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "        self.final_norm = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.size()\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83d9545",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "415599ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModelPT(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.05, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyTorchGPT = GPTModelPT(GPT_CONFIG)\n",
    "PyTorchGPT.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d006e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.2 ms ± 13.2 ms per loop (mean ± std. dev. of 25 runs, 25 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 25 -r 25\n",
    "forward(PyTorchGPT, input_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b483a97",
   "metadata": {},
   "source": [
    "## Add some little optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee46be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_in=cfg['emb_dim'], \n",
    "                                       d_out=cfg['emb_dim'], \n",
    "                                       context_length=cfg['context_length'], \n",
    "                                       dropout=cfg['drop_rate'], \n",
    "                                       num_heads=cfg['n_heads'], \n",
    "                                       qkv_bias=cfg['qkv_bias'])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = nn.LayerNorm(cfg['emb_dim']) #LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = nn.LayerNorm(cfg['emb_dim']) #LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_resid = nn.Dropout(cfg['drop_rate'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_resid(self.attn(self.norm1(x)))\n",
    "        return x + self.drop_resid(self.ff(self.norm2(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98eaa5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModelOPT(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "        self.final_norm = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        _, seq_len = in_idx.size()\n",
    "        return self.out_head(self.final_norm(self.trf_blocks(self.drop_emb(self.tok_emb(in_idx) + self.pos_emb(torch.arange(seq_len, device=in_idx.device))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ab0a2",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cbeb7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModelOPT(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.05, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyTorchGPTOpt = GPTModelOPT(GPT_CONFIG)\n",
    "PyTorchGPTOpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ea9e4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 ms ± 8.44 ms per loop (mean ± std. dev. of 25 runs, 25 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 25 -r 25\n",
    "forward(PyTorchGPTOpt, input_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed5bb7",
   "metadata": {},
   "source": [
    "## Add checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72e6fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_in=cfg['emb_dim'], \n",
    "                                       d_out=cfg['emb_dim'], \n",
    "                                       context_length=cfg['context_length'], \n",
    "                                       dropout=cfg['drop_rate'], \n",
    "                                       num_heads=cfg['n_heads'], \n",
    "                                       qkv_bias=cfg['qkv_bias'])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = nn.LayerNorm(cfg['emb_dim']) #LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = nn.LayerNorm(cfg['emb_dim']) #LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_resid = nn.Dropout(cfg['drop_rate'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_resid(self.attn(self.norm1(x)))\n",
    "        return x + self.drop_resid(self.ff(self.norm2(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d660a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModelOPT(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "        self.final_norm = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        _, seq_len = in_idx.size()\n",
    "        return self.out_head(self.final_norm(self.trf_blocks(self.drop_emb(self.tok_emb(in_idx) + self.pos_emb(torch.arange(seq_len, device=in_idx.device))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0458f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8888776f",
   "metadata": {},
   "source": [
    "## ReversibleBlocks + checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6967505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim),\n",
    "            nn.GELU(), #GELU(),\n",
    "            nn.Linear(4 * emb_dim, emb_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7090893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversibleTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        emb_dim = cfg['emb_dim']//2\n",
    "        self.attn = MultiHeadAttention(d_in=emb_dim, \n",
    "                                       d_out=emb_dim, \n",
    "                                       context_length=cfg['context_length'], \n",
    "                                       dropout=cfg['drop_rate'], \n",
    "                                       num_heads=cfg['n_heads'], \n",
    "                                       qkv_bias=cfg['qkv_bias'])\n",
    "        self.ff = FeedForward(emb_dim)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim) #LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = nn.LayerNorm(emb_dim) #LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_resid = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # reversible update\n",
    "        # y1 = x1 + f(x2)\n",
    "        # y2 = x2 + g(y1)\n",
    "\n",
    "        def f(u):\n",
    "            u = self.norm1(u)\n",
    "            attn_output = self.attn(u)\n",
    "            attn_output = self.drop_resid(attn_output)\n",
    "            return attn_output\n",
    "        \n",
    "        def g(v):\n",
    "            return self.drop_resid(self.ff(self.norm2(v)))\n",
    "        \n",
    "        f_x2 = checkpoint(f, x2)\n",
    "        y1 = x1 + f_x2\n",
    "        g_y1 = checkpoint(g, y1)\n",
    "        y2 = x2 + g_y1\n",
    "\n",
    "        return y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7d1e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModelRev(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[ReversibleTransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "        self.final_norm = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.size()\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # initialize reversible pairs: split features\n",
    "        # split last dim\n",
    "        x1, x2 = torch.chunk(x, 2, dim=-1)  # each (batch_size, seq_len, emb_dim//2)\n",
    "\n",
    "        # Now we change x = self.trf_blocks(x) to: \n",
    "        for layer in self.trf_blocks:\n",
    "            x1, x2 = layer(x1, x2)\n",
    "        # merge\n",
    "        x = torch.cat([x1, x2], dim=-1)  # (b, s, dim)\n",
    "        \n",
    "        # Now as usual\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b580039",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "691a68c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModelRev(\n",
       "  (tok_emb): Embedding(50257, 192)\n",
       "  (pos_emb): Embedding(1024, 192)\n",
       "  (drop_emb): Dropout(p=0.05, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=96, out_features=96, bias=False)\n",
       "        (W_query): Linear(in_features=96, out_features=96, bias=False)\n",
       "        (W_value): Linear(in_features=96, out_features=96, bias=False)\n",
       "        (out_proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=384, out_features=96, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (1): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_key): Linear(in_features=96, out_features=96, bias=False)\n",
       "        (W_query): Linear(in_features=96, out_features=96, bias=False)\n",
       "        (W_value): Linear(in_features=96, out_features=96, bias=False)\n",
       "        (out_proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=384, out_features=96, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=192, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyTorchRev = GPTModelRev(GPT_CONFIG)\n",
    "PyTorchRev.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "810808a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x:\\Users\\jeka_\\miniconda3\\envs\\LLM\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 ms ± 1.69 ms per loop (mean ± std. dev. of 25 runs, 25 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 25 -r 25\n",
    "forward(PyTorchRev, input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826ab1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58dad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a32181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dadc41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af477c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fbfdbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5594a4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e15e514b",
   "metadata": {},
   "source": [
    "Для большой сети (т.е. эксперты с маленьким размером не смогут нормально оубчиться, плюс обучать мое дорого, имеет смысл, только есть есть несколько нод и обучать на каждой ноде своего эксперта, чтобы снизить накладные расходы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae8c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ReversibleBlock(nn.Module):\n",
    "    def __init__(self, f, g):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.g = g\n",
    "\n",
    "    def forward(self, x1, x2, mask=None):\n",
    "        y1 = x1 + self.f(x2, mask)\n",
    "        y2 = x2 + self.g(y1)\n",
    "        return y1, y2\n",
    "\n",
    "    def backward_pass(self, y1, y2):\n",
    "        x2 = y2 - self.g(y1)\n",
    "        x1 = y1 - self.f(x2)\n",
    "        return x1, x2\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, num_experts=4, top_k=1):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        # Experts: feed-forward layers\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, d_ff),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(d_ff, d_model)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "        # Gating network: per-token\n",
    "        self.gate = nn.Linear(d_model, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (seq_len, batch, d_model) or (batch, seq, d_model)\n",
    "        is_seq_first = x.dim() == 3 and x.size(0) != x.size(1)\n",
    "        if is_seq_first:\n",
    "            seq, bsz, d = x.size()\n",
    "            tokens = x.view(-1, d)\n",
    "        else:\n",
    "            bsz, seq, d = x.size()\n",
    "            tokens = x.view(-1, d)\n",
    "        # Gating\n",
    "        gate_scores = F.softmax(self.gate(tokens), dim=-1)  # (tokens, experts)\n",
    "        topk_vals, topk_idx = torch.topk(gate_scores, self.top_k, dim=-1)\n",
    "        # Compute expert outputs and combine\n",
    "        expert_outputs = torch.stack([exp(tokens) for exp in self.experts], dim=1)  # (tokens, experts, d)\n",
    "        # Mask all but top-k\n",
    "        mask = torch.zeros_like(gate_scores)\n",
    "        for i in range(self.top_k):\n",
    "            mask.scatter_(1, topk_idx[:, i:i+1], topk_vals[:, i:i+1])\n",
    "        combined = (expert_outputs * mask.unsqueeze(-1)).sum(dim=1)\n",
    "        # reshape back\n",
    "        if is_seq_first:\n",
    "            return combined.view(seq, bsz, d)\n",
    "        else:\n",
    "            return combined.view(bsz, seq, d)\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1,\n",
    "                 reversible=False, moe_experts=0, moe_topk=1):\n",
    "        super().__init__()\n",
    "        self.reversible = reversible\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        # decide FFN or MoE\n",
    "        if moe_experts > 1:\n",
    "            self.ffn = MoE(d_model, d_ff, num_experts=moe_experts, top_k=moe_topk)\n",
    "        else:\n",
    "            self.ffn = nn.Sequential(\n",
    "                nn.Linear(d_model, d_ff),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_ff, d_model)\n",
    "            )\n",
    "\n",
    "        if reversible:\n",
    "            self.block = ReversibleBlock(\n",
    "                lambda x, mask: self.attn(self.norm1(x), self.norm1(x), self.norm1(x), attn_mask=mask)[0],\n",
    "                lambda x, _: self.ffn(self.norm2(x))\n",
    "            )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        if self.reversible:\n",
    "            x1, x2 = x.chunk(2, dim=-1)\n",
    "            y1, y2 = self.block(x1, x2, mask)\n",
    "            return torch.cat([y1, y2], dim=-1)\n",
    "\n",
    "        # Standard residual\n",
    "        attn_out, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x), attn_mask=mask)\n",
    "        x = x + attn_out\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class GPTReversible(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        num_layers=12,\n",
    "        seq_len=1024,\n",
    "        dropout=0.1,\n",
    "        reversible=True,\n",
    "        shared=False,\n",
    "        moe_experts=0,\n",
    "        moe_topk=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
    "        # instantiate layers once if shared, else new each time\n",
    "        if shared:\n",
    "            layer = TransformerLayer(d_model, num_heads, d_ff, dropout,\n",
    "                                     reversible, moe_experts, moe_topk)\n",
    "            self.layers = nn.ModuleList([layer] * num_layers)\n",
    "        else:\n",
    "            self.layers = nn.ModuleList([\n",
    "                TransformerLayer(d_model, num_heads, d_ff, dropout,\n",
    "                                 reversible, moe_experts, moe_topk)\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "        self.norm_final = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, mask=None):\n",
    "        x = self.token_emb(input_ids) + self.pos_emb[:, :input_ids.size(1)]\n",
    "        if self.layers[0].reversible and x.size(-1) % 2 != 0:\n",
    "            raise ValueError(\"d_model must be even for reversible transformer.\")\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm_final(x)\n",
    "        return self.head(x)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = GPTReversible(\n",
    "        vocab_size=30522,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        num_layers=12,\n",
    "        seq_len=1024,\n",
    "        dropout=0.1,\n",
    "        reversible=True,\n",
    "        shared=True,        # Cross-layer sharing\n",
    "        moe_experts=4,      # Sparse Mixture-of-Experts\n",
    "        moe_topk=1\n",
    "    )\n",
    "    input_ids = torch.randint(0, 30522, (2, 128))\n",
    "    logits = model(input_ids)\n",
    "    print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbd3500",
   "metadata": {},
   "source": [
    "То же самое, но без MoE, подойдет, вероятно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe517ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ReversibleBlock(nn.Module):\n",
    "    def __init__(self, f, g):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.g = g\n",
    "\n",
    "    def forward(self, x1, x2, mask=None):\n",
    "        y1 = x1 + self.f(x2, mask)\n",
    "        y2 = x2 + self.g(y1)\n",
    "        return y1, y2\n",
    "\n",
    "    def backward_pass(self, y1, y2):\n",
    "        x2 = y2 - self.g(y1)\n",
    "        x1 = y1 - self.f(x2)\n",
    "        return x1, x2\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1, reversible=False):\n",
    "        super().__init__()\n",
    "        self.reversible = reversible\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "        if reversible:\n",
    "            self.block = ReversibleBlock(\n",
    "                lambda x, mask: self.attn(self.norm1(x), self.norm1(x), self.norm1(x), attn_mask=mask)[0],\n",
    "                lambda x, _: self.ffn(self.norm2(x))\n",
    "            )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        if self.reversible:\n",
    "            x1, x2 = x.chunk(2, dim=-1)\n",
    "            y1, y2 = self.block(x1, x2, mask)\n",
    "            return torch.cat([y1, y2], dim=-1)\n",
    "\n",
    "        # Standard residual\n",
    "        attn_out, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x), attn_mask=mask)\n",
    "        x = x + attn_out\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class GPTReversible(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        num_layers=12,\n",
    "        seq_len=1024,\n",
    "        dropout=0.1,\n",
    "        reversible=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(d_model, num_heads, d_ff, dropout, reversible)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm_final = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, mask=None):\n",
    "        x = self.token_emb(input_ids) + self.pos_emb[:, :input_ids.size(1)]\n",
    "        # For reversible, input dimension must be even\n",
    "        if self.layers[0].reversible and x.size(-1) % 2 != 0:\n",
    "            raise ValueError(\"d_model must be even for reversible transformer.\")\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm_final(x)\n",
    "        return self.head(x)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = GPTReversible(\n",
    "        vocab_size=30522,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        num_layers=12,\n",
    "        seq_len=1024,\n",
    "        dropout=0.1,\n",
    "        reversible=True\n",
    "    )\n",
    "    input_ids = torch.randint(0, 30522, (2, 128))\n",
    "    logits = model(input_ids)\n",
    "    print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add83d5a",
   "metadata": {},
   "source": [
    "Много кода с реализацией LoRA - адаптера для последующего дообучения (тоже не особо актуально для маленьких моделей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd99fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# LoRA adapter for Linear layers, with optional freezing and zero-initialization of base\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, r=4, alpha=1.0,\n",
    "                 bias=True, freeze_base=False, zero_init_base=False):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        # base weight\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        if zero_init_base:\n",
    "            nn.init.zeros_(self.linear.weight)\n",
    "            if bias:\n",
    "                nn.init.zeros_(self.linear.bias)\n",
    "        if freeze_base:\n",
    "            for param in self.linear.parameters():\n",
    "                param.requires_grad = False\n",
    "        # low-rank adapters\n",
    "        if r > 0:\n",
    "            self.lora_down = nn.Linear(in_features, r, bias=False)\n",
    "            self.lora_up = nn.Linear(r, out_features, bias=False)\n",
    "            nn.init.kaiming_uniform_(self.lora_down.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_up.weight)\n",
    "        else:\n",
    "            self.lora_down = self.lora_up = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        if self.r > 0:\n",
    "            delta = self.lora_up(self.lora_down(x)) * (self.alpha / self.r)\n",
    "            out = out + delta\n",
    "        return out\n",
    "\n",
    "# Example reversible transformer block (for memory optimization)\n",
    "class ReversibleBlock(nn.Module):\n",
    "    def __init__(self, f, g):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.g = g\n",
    "    def forward(self, x1, x2, mask=None):\n",
    "        y1 = x1 + self.f(x2, mask)\n",
    "        y2 = x2 + self.g(y1)\n",
    "        return y1, y2\n",
    "    def backward_pass(self, y1, y2):\n",
    "        x2 = y2 - self.g(y1)\n",
    "        x1 = y1 - self.f(x2)\n",
    "        return x1, x2\n",
    "\n",
    "# Flexible transformer block supporting reversible mode\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1,\n",
    "                 reversible=False, **lora_kwargs):\n",
    "        super().__init__()\n",
    "        self.reversible = reversible\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.MultiheadAttention(d_model, num_heads, dropout=dropout),\n",
    "        )\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        if reversible:\n",
    "            self.block = ReversibleBlock(self.attn, self.ffn)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        if self.reversible:\n",
    "            x1, x2 = torch.chunk(x, 2, dim=-1)\n",
    "            y1, y2 = self.block(x1, x2, mask)\n",
    "            return torch.cat([y1, y2], dim=-1)\n",
    "        # standard residual\n",
    "        attn_out, _ = self.attn[1](self.attn[0](x), self.attn[0](x), self.attn[0](x), attn_mask=mask)\n",
    "        x = x + attn_out\n",
    "        x = x + self.ffn(x)\n",
    "        return x\n",
    "\n",
    "# GPT-like model supporting training from scratch with various optimizations\n",
    "class GPTOptimized(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, d_ff=2048,\n",
    "                 num_layers=12, exit_layers=None, reversible=False,\n",
    "                 shared=False, lora_r=0, lora_alpha=1.0,\n",
    "                 freeze_base=False, zero_init_base=False):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, 1024, d_model))\n",
    "        blocks = []\n",
    "        block_module = TransformerBlock\n",
    "        for _ in range(num_layers):\n",
    "            blocks.append(\n",
    "                block_module(d_model, num_heads, d_ff,\n",
    "                             reversible=reversible,\n",
    "                             r=lora_r, alpha=lora_alpha,\n",
    "                             freeze_base=freeze_base,\n",
    "                             zero_init_base=zero_init_base)\n",
    "            )\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.exit_layers = set(exit_layers or [])\n",
    "        if exit_layers:\n",
    "            self.exit_heads = nn.ModuleDict({\n",
    "                str(l): nn.Linear(d_model, vocab_size) for l in exit_layers\n",
    "            })\n",
    "\n",
    "    def forward(self, input_ids, mask=None, exit_thresh=0.9):\n",
    "        x = self.token_emb(input_ids) + self.pos_emb[:, :input_ids.size(1)]\n",
    "        for idx, block in enumerate(self.blocks, 1):\n",
    "            x = block(x, mask)\n",
    "            if idx in self.exit_layers:\n",
    "                logits = self.exit_heads[str(idx)](x.mean(dim=1))\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                if probs.max(dim=-1).values.mean() > exit_thresh:\n",
    "                    return logits\n",
    "        x = self.ln_final(x)\n",
    "        return self.head(x)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Пример обучения с нуля, reversible=False, full-trainable linear\n",
    "    model = GPTOptimized(\n",
    "        vocab_size=30522,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        num_layers=6,\n",
    "        exit_layers=[3, 6],\n",
    "        reversible=False,\n",
    "        shared=False,\n",
    "        lora_r=0,\n",
    "        lora_alpha=1.0,\n",
    "        freeze_base=False,\n",
    "        zero_init_base=False,\n",
    "    )\n",
    "    input_ids = torch.randint(0, 30522, (2, 128))\n",
    "    logits = model(input_ids)\n",
    "    print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624536d1",
   "metadata": {},
   "source": [
    "Ускоренный (по возможности) инференс и обучение (но пока без рефакторинга)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Fused LayerNorm for speed\n",
    "class FusedLayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = FusedLayerNorm(d_model)\n",
    "        self.norm2 = FusedLayerNorm(d_model)\n",
    "        self.num_heads = num_heads\n",
    "        # Fused QKV projection\n",
    "        self.attn_proj = nn.Linear(d_model, d_model * 3, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        # Feed-forward\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model, bias=False)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def attention(self, x, mask=None):\n",
    "        B, S, D = x.size()\n",
    "        qkv = self.attn_proj(x).reshape(B, S, 3, self.num_heads, D // self.num_heads)\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "        # flash attention\n",
    "        attn = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=self.dropout.p)\n",
    "        attn = attn.transpose(1, 2).reshape(B, S, D)\n",
    "        return self.out_proj(attn)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        def layer_fn(x, mask):\n",
    "            y = x + self.dropout(self.attention(self.norm1(x), mask))\n",
    "            y = y + self.dropout(self.ffn(self.norm2(y)))\n",
    "            return y\n",
    "        return checkpoint(layer_fn, x, mask)\n",
    "\n",
    "class GPTOptimized(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, d_ff=2048,\n",
    "                 num_layers=12, seq_len=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm_final = FusedLayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, mask=None):\n",
    "        x = self.token_emb(input_ids) + self.pos_emb[:, :input_ids.size(1)]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm_final(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# Dummy dataset for language modeling\n",
    "def get_dummy_dataset(vocab_size, seq_len, dataset_size=10000):\n",
    "    class RandomDataset(Dataset):\n",
    "        def __init__(self, size):\n",
    "            self.size = size\n",
    "        def __len__(self):\n",
    "            return self.size\n",
    "        def __getitem__(self, idx):\n",
    "            data = torch.randint(0, vocab_size, (seq_len,), dtype=torch.long)\n",
    "            # labels = next token prediction (shifted)\n",
    "            return data, data\n",
    "    return RandomDataset(dataset_size)\n",
    "\n",
    "\n",
    "def train(\n",
    "    model, dataloader, optimizer, scheduler, device,\n",
    "    epochs=1, grad_accum=1, max_norm=1.0\n",
    "):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        total_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        for step, (inputs, labels) in enumerate(dataloader, 1):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(inputs)\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.view(-1, logits.size(-1)),\n",
    "                    labels.view(-1)\n",
    "                ) / grad_accum\n",
    "            scaler.scale(loss).backward()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if step % grad_accum == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), max_norm\n",
    "                )\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                if scheduler:\n",
    "                    scheduler.step()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch}: avg loss = {avg_loss:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Config\n",
    "    vocab_size = 30522\n",
    "    seq_len = 128\n",
    "    batch_size = 16\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Model\n",
    "    model = GPTOptimized(vocab_size=vocab_size, seq_len=seq_len)\n",
    "    model = torch.compile(model, backend='inductor')\n",
    "    model = model.half().to(device)\n",
    "\n",
    "    # Data\n",
    "    dataset = get_dummy_dataset(vocab_size, seq_len)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Optimizer & scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=10\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    train(\n",
    "        model, dataloader, optimizer, scheduler, device,\n",
    "        epochs=3, grad_accum=2\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
