{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9ad5b2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "479f4b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint, checkpoint_sequential\n",
    "import transformers\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e52c0",
   "metadata": {},
   "source": [
    "Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96cbe8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getenv(\"PATH\")\n",
    "DATAPATH = os.getenv(\"DATAPATH\")\n",
    "PREPARED_DATA_DIR = os.getenv(\"PREPARED_DATA_DIR\")\n",
    "CACHE_DIR = os.getenv(\"CACHE_DIR\")\n",
    "#TOK_NAME = \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\"\n",
    "TOK_NAME = os.getenv(\"TOK_NAME\")\n",
    "PARQUET_DATA_DIR = os.getenv(\"PARQUET_DATA_DIR\")\n",
    "CHECKPOINTS_PATH = os.getenv(\"CHECKPOINTS_PATH\")\n",
    "NPY_DATA_DIR = os.getenv(\"NPY_DATA_DIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d437f51",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9db08731",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "    'vocab_size': 50257, # in 151670 (if you use tokenizer.vocab_size then you get partial vocab_size without added tokens)\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 256, #768\n",
    "    'n_heads': 32,#12,\n",
    "    'n_layers': 32,#12,\n",
    "    'drop_rate': 0.05, # 0l1\n",
    "    'qkv_bias': False,\n",
    "    'num_segments': 2,\n",
    "    'initializer_range': 0.02\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98fcc0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if (torch.cuda.is_available()) else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe7364",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee0e4a",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e222b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = transformers.AutoTokenizer.from_pretrained(TOK_NAME, cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c14940",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.pad_token = '<|pad|>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b58c7c7",
   "metadata": {},
   "source": [
    "### Check tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "febbce4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|endoftext|>': 50256}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "056fce04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|endoftext|>', '<|endoftext|>')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.eos_token, tok.bos_token, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5edd7623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb2b7e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|pad|>', '<|endoftext|>', '<|endoftext|>')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.pad_token, tok.eos_token, tok.bos_token, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad58e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If tokenizer dont have pad_token\n",
    "#tok.pad_token = tok.eos_token # Not, it not best idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "394abeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  140,   253, 21169, 18849, 38857, 16843, 20375,    11, 12466,   118,\n",
       "         16142, 31583, 12466,   112, 16843, 30143, 16142,   285,    71,    73,\n",
       "            76,   220, 50256,    27,    91, 15636,    91,  6927,    91, 15636,\n",
       "            91,  6927,    91, 15636,    91,  6927,    91, 15636,    91,    29,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok('Привет, как дела mhjm <|endoftext|><|pad|><|pad|><|pad|><|pad|>', return_tensors='pt', padding='max_length', max_length=100)['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82898a5c",
   "metadata": {},
   "source": [
    "## Iterative version of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3cdc0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetV3(Dataset):\n",
    "    def __init__(self, dataframe: str, tokenizer: object, max_length: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        for i, curr_chunk in tqdm(dataframe.iterrows(), total=dataframe.shape[0]):\n",
    "            token_ids = tokenizer(curr_chunk['Sample'], return_tensors='pt', padding='max_length', max_length=max_length+1)['input_ids']\n",
    "            input_chunk = token_ids[:,:max_length].view(-1)\n",
    "            target_chunk = token_ids[:,1:max_length+1].view(-1)\n",
    "            #print(input_chunk.size(), target_chunk.size(),)\n",
    "            self.input_ids.append(input_chunk)\n",
    "            self.target_ids.append(target_chunk)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8af056",
   "metadata": {},
   "source": [
    "### Load actual data and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76cccaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = pd.read_parquet(PARQUET_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5480f9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224662, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_parquet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "989757da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b107eec214b941938eec9583f0a1acb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_cd = CustomDatasetV3(dataframe=data_parquet.iloc[:100000], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])\n",
    "#train_cd = CustomDatasetV3(dataframe=data_parquet.iloc[:100], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2de0dbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4acf7252a014ad4bf6565a2a0b674bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_cd = CustomDatasetV3(dataframe=data_parquet.iloc[-10000:], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])\n",
    "#val_cd = CustomDatasetV3(dataframe=data_parquet.iloc[-100:], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb02d6",
   "metadata": {},
   "source": [
    "batch_size maybe 8 or 12 (or 16) check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96022e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(dataset=train_cd, batch_size=12, shuffle=True, num_workers=0) # num_workers=2 don't work?\n",
    "val_data = DataLoader(dataset=val_cd, batch_size=12, shuffle=True, num_workers=0) # num_workers=2 don't work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "081aeea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[12466,   251, 16843,  ..., 35072, 21169, 18849],\n",
       "         [  198,   198,   140,  ..., 35072,   141,   229],\n",
       "         [  198,   198,   140,  ..., 16843, 21169,   141],\n",
       "         ...,\n",
       "         [12466,   239, 45035,  ...,   198,   198,   140],\n",
       "         [  198,   198,   140,  ...,   141,   229, 16142],\n",
       "         [12466,   252, 22177,  ..., 15166, 18849,   141]]),\n",
       " tensor([[  251, 16843, 21727,  ..., 21169, 18849, 15166],\n",
       "         [  198,   140,   239,  ...,   141,   229, 18849],\n",
       "         [  198,   140,   246,  ..., 21169,   141,   227],\n",
       "         ...,\n",
       "         [  239, 45035, 30143,  ...,   198,   140,    94],\n",
       "         [  198,   140,   240,  ...,   229, 16142, 16843],\n",
       "         [  252, 22177,   220,  ..., 18849,   141,   227]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8b1e00",
   "metadata": {},
   "source": [
    "## Summed version of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7198d610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel # import pandarallel\n",
    "pandarallel.initialize() # initialize pandarallel\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dd06013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomTextProcerssing(dataframe, eos_token):#tok.pad_token, tok.eos_token, tok.bos_token, \n",
    "    sum_text = \"\"\n",
    "    for i, curr_chunk in tqdm(dataframe.iterrows(), total=dataframe.shape[0]):\n",
    "        sum_text += curr_chunk['Sample'] + eos_token\n",
    "    return sum_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d58ef605",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = pd.read_parquet(PARQUET_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "960dc169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6412a1479b0474b9e3b19ccb117765d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/224662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m sum_txt = \u001b[43mCustomTextProcerssing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_parquet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mCustomTextProcerssing\u001b[39m\u001b[34m(dataframe, eos_token)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mCustomTextProcerssing\u001b[39m(dataframe, eos_token):\u001b[38;5;66;03m#tok.pad_token, tok.eos_token, tok.bos_token, \u001b[39;00m\n\u001b[32m      2\u001b[39m     sum_text = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43msum_text\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_chunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSample\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43meos_token\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sum_text\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\Users\\jeka_\\miniconda3\\envs\\LLM\\Lib\\site-packages\\tqdm\\notebook.py:252\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[32m    251\u001b[39m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:  \u001b[38;5;66;03m# NOQA\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "sum_txt = CustomTextProcerssing(data_parquet, tok.eos_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d9c5d",
   "metadata": {},
   "source": [
    "Все сразу сделать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fadc4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet['Sample'] = data_parquet['Sample'] + tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68cd966b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Литва́ ( ), официальное название — Лито́вская ...\n",
       "1          В 1422 году в состав великого княжества оконч...\n",
       "2         \\n\\nВ 1919 году в Литве введена должность през...\n",
       "3          Блокада продолжалась 74 дня и прекратилась по...\n",
       "4         \\n\\nВнутренняя политика \\n\\nВ июне 2008 года п...\n",
       "                                ...                        \n",
       "224657     В постсоветских изданиях за клубом закрепилос...\n",
       "224658    \\n\\n1974 год становится для «Баварии» и её игр...\n",
       "224659     Но в четвертьфинала путь «Баварии» преградила...\n",
       "224660     6 апреля 2013 года, за шесть туров до окончан...\n",
       "224661     Оба гола были забиты в дополнительное время т...\n",
       "Name: Sample, Length: 224662, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_parquet['Sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0afdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summed = data_parquet['Sample'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a36676d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Литва́ ( ), официальное название— Лито́вская Респу́блика ()— государство, расположенное в Северной Европе Площадь—  км² Протяжённость с севера на юг— 280км, а с запада на восток— 370 км Население составляет  человек (август, 2023) Занимает 137-е место в мире по численности населения и 121-е по территории Имеет выход к Балтийскому морю, расположена на его восточном побережье Береговая линия составляет всего 99 км (наименьший показатель среди государств Балтии) На севере граничит с Латвией, на юго-востоке— с Белоруссией, на юго-западе— с Польшей и Калининградской областью России По площади и населению является самым крупным государством из стран БалтииСтолица— Вильнюс Официальный язык— литовский Денежная единица— евроВосстановление независимости страны провозглашено 11 марта 1990года 6 сентября 1991года Государственный совет СССР признал независимость Литвы Литва— член ООН (1991), ОБСЕ (1991), Совета Европы (1993), ВТО (2001), Европейского союза (2004), НАТО (2004) и ОЭСР (2018) Входит в Шенгенскую зону и ЕврозонуЭтимология Этимология слова «Литва» точно не известна, при этом существует множество версий, ни одна из которых не получила всеобщего признания Корень «лит» и его варианты «лет»/«лют» допускают различные толкования как в балтских и славянских, так и в других индоевропейских языках Так, например, существуют созвучные топонимы на территории Словакии «Lytva» и Румынии «Litua», известные с XI—XIIвеков По мнению ЕПоспелова, топоним образован от древнего названия реки Летава (Lietavà от  «лить», русское «Летаука») Феодальное княжество, по землям которого протекала эта река, со временем заняло ведущее положение и название было распространено на всё государство В «Повести временных лет» (XIIвек) упоминается этноним «литва», полностью совпадающий с названием местности «Литва» и по смыслу (территория, где живёт литва), и по формеГеография Поверхность равнинная, со следами древнего оледенения Поля и луга занимают 57% территории, леса и кустарники— 30%, болота— 6%, внутренние воды— 1%Высшая точка— 293,84 м над уровнем моря— холм Аукштояс (или Аукштасис калнас) в юго-восточной части страны, в 23,5км от ВильнюсаКрупнейшие реки— Неман и ВилияБолее 3тыс озёр (1,5% территории): крупнейшее из них— Друкшяй на границе Латвии, Литвы и Белоруссии (площадь 44,8 км²), самое глубокое— Таурагнас, 61 м), самое длинное— Асвея длиной в 30км у местечка ДубингяйКлимат переходный от морского к континентальному Средняя температура зимой −5\\u202f°C, летом +17\\u202f°C Выпадает 748мм осадков в годПолезные ископаемые: торф, минеральные материалы, строительные материалыИсторияДревнейшая история Территория современной Литвы была заселена людьми с конца X—IX тысячелетия донэ Жители занимались охотой и рыболовством, использовали лук и стрелы с кремнёвыми наконечниками, скребки для обработки кожи, удочки и сети В конце неолита (III—II тысячелетия донэ) на территорию современной Литвы проникли индоевропейские племена Они занимались земледелием и скотоводством, при этом охота и рыболовство оставались основными занятиями местных жителей вплоть до широкого распространения железных орудий труда Индоевропейцы, заселившие земли между устьями Вислы и Западной Двины, выделились в отдельную группу, названную учёными балтамиТрадиционно считается, что этническая основа Литвы сформирована носителями археологической культуры восточнолитовских курганов, сложившейся в Vвеке нэ на территории современных Восточной Литвы и Северо-Западной Белоруссии Около VIIвека литовский язык отделился от латышскогоЗарождение государства Становление государственности относят к XIIIвеку, при этом само название «Литва» впервые упомянуто в Кведлинбургских анналах под 1009годом в сообщении об убийстве язычниками миссионера Бруно на границе Руси и Литвы ( → косвп ) По наиболее распространённой, но аргументированно опровергнутой версии, топоним возник от названия небольшой реки Летаука, притока Няриса Согласно более современной гипотезе, название страны могло произойти от этнонима «леты» или «лейти», которым жители окрестных земель называли дружинников литовских князейВ начале XIIIвека в земли балтов-язычников с запада началось вторжение немецких рыцарей-крестоносцев Они покорили Пруссию и Ливонию В это же время с юга началась экспансия Галицко-Волынского княжества К середине XIIIвека многие литовские земли были объединены под властью князя Миндовга, принявшего в 1251году католическое крещение и коронованного в 1253году Через несколько лет Миндовг отрёкся от христианства, и до конца XIVвека литовские земли оставались языческими Несмотря на то, что уже в 1263году Миндовг был свергнут, его правление положило начало более чем пятисотлетнему существованию Великого княжества ЛитовскогоВ Великом княжестве Литовском В XIV— начале XVвеках территория Великого княжества Литовского стремительно росла, в основном за счёт присоединения земель Западной Руси Включение в состав государства густо населённых православными русинами обширных территорий, многократно превышающих по площади и количеству населения собственно литовские земли, привело к перениманию литовскими князьями, ставшими наместниками на русских землях, православной культуры, а западнорусский язык стал официальным языком ВКЛ Собственно литовский язык до XVIвека оставался бесписьменным, хотя и продолжал использоваться на этнически литовских земляхВ 1385году великий князь литовский Ягайло заключил Кревскую унию с Королевством Польским По условиям унии, Ягайло обязался крестить литовские земли по католическому обряду, жениться на польской королеве Ядвиге, а сам становился королём Польши и сохранял титул великого князя литовского Однако вскоре он вынужден был уступить власть в Великом княжестве Литовском своему двоюродному брату Витовту Последний, хотя и признал себя вассалом Ягайло, проводил самостоятельную внешнюю политику, и таким образом полное объединение государств не состоялось В годы правления Витовта (1392—1430) Великое княжество Литовское достигло наивысшего расцвета, а его территория составила примерно 930тыс км² В 1422году в состав великого княжества окончательно вошла Жемайтия, долгое время служившая основным предметом споров с крестоносцамиВеликий князь Казимир, одновременно бывший и королём польским, расширил влияние династии Ягеллонов— подчинил Пруссию, посадил своего сына Владислава на чешский и венгерский троны В 1492—1526годах существовала политическая система государств Ягеллонов, охватывавшая Польшу (с вассалами Пруссией и Молдавским княжеством), Великое княжество Литовское, Чехию и ВенгриюПравовой основой государства являлся стату́т, изданный в трёх редакциях (1529, 1566, 1588), отражающих социально-экономические и политические изменения Статут регламентировал вопросы гражданского, уголовного и процессуального права На некоторых территориях бывшего Великого княжества третья редакция статута действовала до 1840годаВ Речи Посполитой В 1569году в Люблине была заключена уния с Польшей, в результате которой образована Речь Посполитая Согласно акту Люблинской унии Литвой и Польшей правил совместно избираемый монарх, а государственные дела решались в общем Сейме Однако правовые системы, армия и администрация оставались раздельнымиВ XVI—XVIIIвеках в Литве по польскому образцу сложилась политическая система, известная как шляхетская демократия Она характеризовалась наличием широких прав шляхты (дворянства) в управлении государством Одновременно с этим происходила полонизация шляхты, выраженная в перенимании правящим сословием Великого княжества Литовского польского языка, культуры и идентичности На непривилегированные сословия полонизация столь значительного влияния не оказала В XVIIIвеке в результате опустошительных войн и всеобъемлющего государственного кризиса Речь Посполитая пришла к упадку и попала под влияние Российской империи 3 мая 1791года сейм Речи Посполитой принял конституцию, согласно которой Речь Посполитая признавалась унитарным государством Взаимная гарантия обоих народов, ставшая частью конституции 3 мая 1791года, гарантировала единство и неделимость Польши и Великого княжества Литовского в едином государстве Конституция действовала до 1793годаВ составе Российской империи В 1772, 1793 и 1795годах состоялись разделы Речи Посполитой между Россией, Пруссией и Австрией Почти вся территория бывшего Великого княжества Литовского была присоединена к Российской империиВ попытках восстановить государственность польско-литовское дворянство поддержало Наполеона в 1812году, а также неоднократно поднимало восстания (1830—1831, 1863—1864), которые, однако, окончились поражением В стремлении ликвидировать польское влияние на территории Великого княжества Литовского российские власти предприняли широкую кампанию деполонизации и русификации В 1864году в Литве была частично запрещена литовская печать латиницей Литовское население, особенно католическое духовенство, сопротивлялись русификации: кириллические издания игнорировали, а книги, напечатанные латиницей, книгоноши нелегально ввозили из соседней Пруссии В 1904году запрет на литовскую латиницу был отменёнВо время Первой мировой войны С первых дней Первой мировой войны территории Литвы оказалась зоной боевых действий между армиями России и Германии К концу 1915года все этнически литовские земли были заняты армией Германской империи Были даже запрещены литовские периодические издания На занятых территориях германская военная администрация вводила новые налоги Однако литовская интеллигенция попыталась воспользоваться геополитической ситуацией и начала искать возможности для провозглашения независимости Литвы В сентябре 1917года в Вильне была проведена конференция, во время которой была избрана Литовская Тариба («Совет Литвы») В ходе конференции было принято решение о необходимости создания независимого литовского государства в этнографических границах и со столицей в Вильнюсе Председателем Совета был избран А Сметона11 декабря 1917года, а после ещё раз— 16 февраля 1918года— было провозглашено восстановление Литовского государства В отличие от принятой 11 декабря под диктовку германских властей декларации, документ от 16 февраля говорит о полной независимости ЛитвыОднако документ от 16 февраля даёт самостоятельность только «на бумаге» После заключения Брест-Литовского мирного договора Германия игнорирует декларацию 16 февраля и, ссылаясь на резолюцию 11 декабря, взвешивает возможность создать Литовское королевство с германским монархом13 июля 1918года Государственный Совет принял решение установить в Литве конституционную монархию и предложить вюртембергскому принцу Вильгельму фон Ураху корону Впрочем, 2 ноября 1918года это решение было отозвано Были приняты основные положения Временной конституции Литвы 11 ноября 1918года Президиум Государственного Совета утвердил первое временное правительство Литвы из шести министров под руководством Аугустинаса Вольдемараса, тем самым дав начало созданию государственного аппарата ЛитвыЛитовская Республика После ухода основных немецких частей и начала боёв с Красной армией 16 декабря 1918года была образована Литовская Советская Республика 27 февраля 1919года в Вильнюсе состоялось объединённое заседание ЦИКов Литвы и Белоруссии, где было провозглашено образование Литовско-Белорусской ССР (Литбел)В феврале— марте 1919года войска литовской Тарибы, поддержанные немецкими гарнизонами, начали военные действия против Литбела, в апреле 1919года к ним присоединилась польская армия В результате территория Литбела была занята польскими частями 12 июля 1920года в Москве был заключён советско-литовский договор Литбел прекратил своё существование, Советская Россия признала независимость Литвы и передала ей спорный Виленский крайПосле поражения Красной армии под Варшавой и советского отступления польские части под командованием генерала Л Желиговского инсценировали мятеж и якобы самовольно заняли территорию Виленского края 12 октября 1920года было объявлено о создании на территории края государства Срединная Литва, однако уже в 1922году оно вошло в состав Польской Республики в качестве воеводстваВ 1919году в Литве введена должность президента, первым президентом государства был избран Антанас Сметона 5 мая 1920года состоялось первое заседание демократически избранного Учредительного Сейма В 1921году страна была принята в Лигу Наций В 1922году была принята постоянная конституция Приведены реформы в области земельных ресурсов, финансов и образования, введена национальная валюта (лит), открыт Литовский университетКлайпедский край (Мемельланд), населённый в основном прусскими литовцами и немцами, по решению Лиги Наций находился под временным управлением французской администрации В 1923году в результате восстания местных литовцев при негласной поддержке литовских властей Клайпедский край был присоединён к Литве на правах автономии 16 февраля 1923года страны Антанты признали присоединение Клайпедского края к ЛитвеВ декабре 1926года в Литве произошёл военный переворот, вернувший к власти лидера националистов А Сметону Началась так называемая авторитарная фаза управления государством В 1927году был распущен парламент В 1928 и 1938годах были приняты конституции, расширяющие президентские полномочия Оппозиционные партии были запрещены, цензура ужесточена, а права национальных меньшинств урезаны17 марта 1938года Польша предъявила Литве ультиматум с требованием установить дипломатические отношения и признать Виленский край неотъемлемой частью польского государства Год спустя, 20 марта 1939года, Литва получила немецкий ультиматум с требованием вернуть ей Клайпедский край Оба ультиматума Литва была вынуждена принятьВторая мировая война и присоединение к СССР Согласно секретному протоколу к заключённому в августе 1939года пакту Молотова— Риббентропа, Литва была включена в сферу интересов Германии 1 сентября Германия вторглась в Польшу, а 17 сентября СССР осуществил освобождение земель, завоёванных Польшей в ходе польско-советской войны, по итогам которого вернул западные земли Белоруссии и Украины, в том числе и Вильнюс25 сентября СССР инициировал переговоры об отказе Германии от претензий на Литву в обмен на территории Варшавского и Люблинского воеводств Польши 10 октября 1939года в Москве был подписан «Договор о передаче Литве города Вильно и Виленской области и о взаимопомощи между СССР и Литвой с конфиденциальным протоколом к нему»14 июня 1940года Литве была предъявлена Советская нота с обвинением в несоблюдении советско-литовского договора о взаимопомощи и в агрессивных намерениях СССР потребовала преобразования правительства с включением в него дружественных СССР политиков, а также согласия на ввод дополнительных контингентов КраснойАрмии в Литву 15 июня Литва согласилась на советские требования 17 июня было создано новое правительство во главе Ю Палецкисом Состав кабинета согласовывается с советским представителем В Деканозовым14—15 июля 1940года, после ввода дополнительного советского военного контингента, в Литве были проведены выборы в Народный сейм, к участию в которых был допущен лишь просоветский «Блок трудового народа» 21 июля Народный сейм провозгласил образование Литовской ССР, 3 августа 1940года она была принята в состав СССР В 1940году, уже будучи в составе СССР, Литва получила часть территории Советской Белоруссии14-18 июня 1941г, менее чем за неделю до нацистского вторжения, около 17000 жителей Литвы были депортированы в Сибирь, многие из которых погибли (см Июньскую депортацию)22 июня 1941года, после нападения Германии на СССР, последовали антисоветские выступления в Литве В Каунасе было провозглашено Временное правительство Литвы, поддерживавшее тесные контакты с нацистами Однако после начала немецкой оккупации это Временное правительство было распущено, а территория Литвы включена в рейхскомиссариат Остланд (генеральный округ Литва) Оккупационную администрацию возглавлял генерал Пятрас Кубилюнас В годы немецкой оккупации нацистами и коллаборационистами было убито около 200000 евреев (около 95% евреев, проживавших в Литве до начала войны)В 1944году нацисты были изгнаны Красной армией с территории Литовской ССР (см Белорусская операция (1944))Послевоенный период Массовые депортации в Сибирь возобновились и продолжались до смерти Сталина в 1953г Антанас Снечкус, лидер Коммунистической партии Литвы с 1940 по 1974год руководил арестами и депортациями Восстановительный период в Литве был осложнён возобновившейся деятельностью националистических вооружённых формирований «лесных братьев» Около 50000 литовцев ушли в леса и с оружием в руках сражались с советскими войсками На более поздних этапах партизанской войны «лесные братья» сформировали «Союз борцов за освобождение Литвы», и его лидер Йонас Жемайтис (кодовое имя Витаутас) был посмертно в 2009году признан президентом Литвы Литовские суды и ЕСПЧ рассматривают борьбу с литовскими партизанами как акт геноцидаПозднее советские власти столкнулись с ненасильственным сопротивлением местной националистической интеллигенции и католического духовенства Часть интеллигенции включилась в активную правозащитную и диссидентскую деятельность В 1976году возникла Литовская Хельсинкская группа В советский период развернулась индустриализация Литвы Связанная с индустриализацией урбанизация республики сопровождалась ростом числа людей умственного труда, повышением образовательного уровня населения (в 1956 число студентов в вузах увеличилось более чем в 4 раза по сравнению с довоенным уровнем и достигло 25000 человек) В 1975году началось строительство Игналинской АЭС и города-спутника Снечкус (в 1992году переименован в Висагинас)В годы перестройки движение за независимость Литвы значительно усилилось и находило всё больше поддержки со стороны местных властей В 1989году была организована акция «Балтийский путь», на которой жители Литвы, Латвии и Эстонии, выражая своё желание выйти из состава СССР, выстроили живую цепь длиной почти в 600кмВосстановление независимости 11 марта 1990года Верховный Совет объявил о восстановлении независимости Литвы Литва стала первой республикой, объявившей о выходе из СССР'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\n",
    "        r'[\\u0000-\\u001F\\u007F\\u0080-\\u009F\\u00A0\\u00AD\\u200B\\u200C\\u200D\\uFEFF]',\n",
    "        '',\n",
    "        data_parquet['Sample'].iloc[:3].sum()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356d1f6b",
   "metadata": {},
   "source": [
    "## New dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afebab2",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6800719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = pd.read_parquet(PARQUET_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet['Sample'] = data_parquet['Sample'] + tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae22c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unicode(x):\n",
    "    return re.sub(r'[\\u0000-\\u001F\\u007F\\u00800-\\u009F\\u00A0\\u0AD\\u200B\\u200C\\u200D\\uFEFF]', '', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83612980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = re.sub(\n",
    "#         r'[\\u0000-\\u001F\\u007F\\u0080-\\u009F\\u00A0\\u00AD\\u200B\\u200C\\u200D\\uFEFF]',\n",
    "#         '',\n",
    "#         data_parquet['Sample'].iloc[:3].sum()\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "603cecb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Литва́ ( ), официальное название— Лито́вская Р...\n",
       "1     В 1422году в состав великого княжества оконча...\n",
       "2    В 1919году в Литве введена должность президент...\n",
       "Name: Sample, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_parquet['Sample'].iloc[:3].apply(replace_unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeba3032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Литва́ ( ), официальное название — Лито́вская ...\n",
       "1     В 1422 году в состав великого княжества оконч...\n",
       "2    \\n\\nВ 1919 году в Литве введена должность през...\n",
       "Name: Sample, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_parquet['Sample'].iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8df0586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551863db84464426aa9e034f0d9fe318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/224662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_parquet['Sample'] = data_parquet['Sample'].progress_apply(replace_unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db64cdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:\\\\My_files\\\\Programming\\\\My_projects\\\\LLM\\\\GPT-like_trained\\\\Data\\\\Processed\\\\Parquet\\\\data_6144symb_max.parquet'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARQUET_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cb543b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet.to_parquet(\"G:\\\\My_files\\\\Programming\\\\My_projects\\\\LLM\\\\GPT-like_trained\\\\Data\\\\Processed\\\\Parquet\\\\data_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8ade83",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa054d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = pd.read_parquet(\"G:\\\\My_files\\\\Programming\\\\My_projects\\\\LLM\\\\GPT-like_trained\\\\Data\\\\Processed\\\\Parquet\\\\data_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32058d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = np.array([], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc3dabed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7027b9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b89a82e2506471f825ded5aa68bb725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/224662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, curr_chunk \u001b[38;5;129;01min\u001b[39;00m tqdm(data_parquet.iterrows(), total=data_parquet.shape[\u001b[32m0\u001b[39m]):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     tokenized_data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_chunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSample\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\Users\\jeka_\\miniconda3\\envs\\LLM\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:5685\u001b[39m, in \u001b[36mappend\u001b[39m\u001b[34m(arr, values, axis)\u001b[39m\n\u001b[32m   5683\u001b[39m     values = ravel(values)\n\u001b[32m   5684\u001b[39m     axis = arr.ndim-\u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5685\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for i, curr_chunk in tqdm(data_parquet.iterrows(), total=data_parquet.shape[0]):\n",
    "    tokenized_data = np.append(tokenized_data, tok(curr_chunk['Sample'])['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93b3b541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112331"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_parquet.shape[0]//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a6072a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = []\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b56be382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37781e64e8614a1e85e59e5a99ee13ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, curr_chunk in tqdm(data_parquet.iloc[:data_parquet.shape[0]//3].iterrows(), total=data_parquet.shape[0]//3):\n",
    "    tokenized_data.extend(tok(curr_chunk['Sample'])['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "161bf009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462551521"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f693d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1-3.npy', 'wb') as f:\n",
    "    np.save(f, tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28a486fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224662"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_parquet.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2071e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74887, 149774, 224662)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_parquet.shape[0]//3, 2*data_parquet.shape[0]//3, 3*data_parquet.shape[0]//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "519f8292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74887, 149774)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_ = data_parquet.shape[0]//3\n",
    "to_ = 2*data_parquet.shape[0]//3\n",
    "from_, to_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aab52631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74887"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_ - from_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e9d1d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f09bb92eb62465dade9952a9ec2d6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6456 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = []\n",
    "for i, curr_chunk in tqdm(data_parquet.iloc[from_:to_].iterrows(), total=to_ - from_):\n",
    "    tokenized_data.extend(tok(curr_chunk['Sample'])['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de3db708",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2-3.npy', 'wb') as f:\n",
    "    np.save(f, tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58c2ca2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149774, 224662)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_ = 2*data_parquet.shape[0]//3\n",
    "to_ = 3*data_parquet.shape[0]//3\n",
    "from_, to_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29ffa86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = []\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "802e64a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189b747305494fd8bb2ac04b535fe6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6401 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = []\n",
    "for i, curr_chunk in tqdm(data_parquet.iloc[from_:to_].iterrows(), total=to_ - from_):\n",
    "    tokenized_data.extend(tok(curr_chunk['Sample'])['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76f6a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('3-3.npy', 'wb') as f:\n",
    "    np.save(f, tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f661d69d",
   "metadata": {},
   "source": [
    "### Load numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8459c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(NPY_DATA_DIR, '1-3.npy'), 'rb') as f:\n",
    "    n1 = np.load(f)\n",
    "#with open('2-3.npy', 'rb') as f:\n",
    "#    n2 = np.load(f)\n",
    "#with open('3-3.npy', 'rb') as f:\n",
    "#    n3 = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62bee967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat([pd.DataFrame(n1), pd.DataFrame(n2)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7d0ab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetV4(Dataset):\n",
    "    def __init__(self, mas: list, max_length: int):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        self.max_length = max_length\n",
    "\n",
    "        for idx in tqdm(range(0, len(mas), self.max_length), total=(len(mas)//self.max_length)):\n",
    "            #token_ids = tokenizer(curr_chunk['Sample'], return_tensors='pt', padding='max_length', max_length=max_length+1)['input_ids']\n",
    "            input_chunk = torch.tensor(mas[idx:idx+max_length]).view(-1)\n",
    "            target_chunk = torch.tensor(mas[idx+1:idx+max_length+1]).view(-1)\n",
    "            #print(input_chunk.size()[0], target_chunk.size()[0],)\n",
    "            if ((input_chunk.size()[0] == self.max_length) or (target_chunk.size()[0] == self.max_length)):\n",
    "                self.input_ids.append(input_chunk)\n",
    "                self.target_ids.append(target_chunk)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faca5a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462551521,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee259603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a10624e12f945d4b138b5e46a542b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/451661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_cd = CustomDatasetV4(mas=n1[:-50000], max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])\n",
    "#train_cd = CustomDatasetV3(dataframe=data_parquet.iloc[:100], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f1130c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c213c68325c4048a28aaaacbbc0b9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_cd = CustomDatasetV4(mas=n1[-50000:], max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])\n",
    "#val_cd = CustomDatasetV3(dataframe=data_parquet.iloc[-100:], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8baec0",
   "metadata": {},
   "source": [
    "batch_size maybe 8 or 12 (or 16) check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68bef91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(dataset=train_cd, batch_size=16, shuffle=True, num_workers=0, drop_last=True) # num_workers=2 don't work? batch_size=12\n",
    "val_data = DataLoader(dataset=val_cd, batch_size=16, shuffle=True, num_workers=0, drop_last=True) # num_workers=2 don't work? batch_size=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb508098",
   "metadata": {},
   "outputs": [],
   "source": [
    "del n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d419c5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cc725e0ab447d3aaf0d8a0f6f106c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[None for _ in tqdm(val_data)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb62085",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.decode(next(iter(train_data))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del n1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18d2ad0",
   "metadata": {},
   "source": [
    "# LLM Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39c86f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionDP_QKV(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0)\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = dropout#nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.size()\n",
    "        qkv = self.W_qkv(x) # b, num_tokens, 3 * self.d_out\n",
    "        queries, keys, values = qkv.split(self.d_out, dim=2)\n",
    "\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        queries = queries.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "        keys = keys.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "        values = values.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "\n",
    "        # All code below we replace with torch.nn.functional.scaled_dot_product_attention\n",
    "        context_vec = torch.nn.functional.scaled_dot_product_attention(queries, keys, values, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "\n",
    "        # att_scores = queries @ keys.transpose(2, 3) # shapes = (num_tokens, self.head_dim) @ (self.head_dim, num_tokens) -> (num_tokens, num_tokens)\n",
    "        # mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        # att_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        # att_weights = torch.softmax(att_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        # att_weights = self.dropout(att_weights)\n",
    "        # context_vec = (att_weights @ values).transpose(1, 2) # (num_tokens, num_tokens) @ (num_tokens, self.head_dim) -> (num_tokens, self.head_dim) -> transpose(1,2) of (b, self.num_heads, num_tokens, self.head_dim) ->\n",
    "        # # -> (b, num_tokens, self.num_heads, self.head_dim) as view in previous code after inference of Linear layers\n",
    "        \n",
    "        # Reshape etc\n",
    "        context_vec = context_vec.transpose(1, 2).contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8026aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mha_dp_qkv = MultiHeadAttentionDP_QKV(d_in=embed_dim, d_out=embed_dim, context_length=context_len, dropout=dropout, num_heads=num_heads, qkv_bias=qkv_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0973dd6a",
   "metadata": {},
   "source": [
    "## Additional classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fbc2483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim),\n",
    "            nn.Mish(), #GELU(),\n",
    "            nn.Linear(4 * emb_dim, emb_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1be1a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversibleTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        emb_dim = cfg['emb_dim']//2\n",
    "        self.attn = MultiHeadAttentionDP_QKV(d_in=emb_dim, \n",
    "                                       d_out=emb_dim, \n",
    "                                       context_length=cfg['context_length'], \n",
    "                                       dropout=cfg['drop_rate'], \n",
    "                                       num_heads=cfg['n_heads'], \n",
    "                                       qkv_bias=cfg['qkv_bias'])\n",
    "        self.ff = FeedForward(emb_dim)\n",
    "        self.norm1 = nn.RMSNorm(emb_dim)#nn.LayerNorm(emb_dim) #LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = nn.RMSNorm(emb_dim)#nn.LayerNorm(emb_dim) #LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_resid = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # reversible update\n",
    "        # y1 = x1 + f(x2)\n",
    "        # y2 = x2 + g(y1)\n",
    "\n",
    "        def f(u):\n",
    "            u = self.norm1(u)\n",
    "            attn_output = self.attn(u)\n",
    "            attn_output = self.drop_resid(attn_output)\n",
    "            return attn_output\n",
    "        \n",
    "        def g(v):\n",
    "            return self.drop_resid(self.ff(self.norm2(v)))\n",
    "        \n",
    "        f_x2 = checkpoint(f, x2, use_reentrant=False, preserve_rng_state=True) # use_reentrant=False for effeciency, preserve_rng_state=True because of dropout\n",
    "        y1 = x1 + f_x2\n",
    "        g_y1 = checkpoint(g, y1, use_reentrant=False, preserve_rng_state=True) # use_reentrant=False for effeciency, preserve_rng_state=True because of dropout\n",
    "        y2 = x2 + g_y1\n",
    "\n",
    "        return y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71018848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModelRev(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.content_size = cfg['context_length']\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(self.content_size, cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "        \n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[ReversibleTransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "        self.final_norm = nn.RMSNorm(cfg['emb_dim'])#nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "        # Weight tying, reference here: https://paperswithcode.com/method/weight-tying\n",
    "        self.tok_emb.weight = self.out_head.weight\n",
    "\n",
    "        # init all weights\n",
    "        #self.apply(self._init_weights)\n",
    "        #self.apply(lambda current_model: self._init_weights2(current_model, initializer_range=cfg['initializer_range']))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        # function from Karpaty's guide\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def _init_weights2(self, module: nn.Module, initializer_range: float = 0.02):\n",
    "        \"\"\"\n",
    "        Инициализация весов в стиле GPT-2:\n",
    "        - Нормальное распределение N(0, initializer_range)\n",
    "        - biases = 0\n",
    "        - LayerNorm.weight = 1, LayerNorm.bias = 0\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=initializer_range)\n",
    "            if getattr(module, \"bias\", None) is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.weight.data.fill_(1.0)\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.size()\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # initialize reversible pairs: split features\n",
    "        # split last dim\n",
    "        x1, x2 = torch.chunk(x, 2, dim=-1)  # each (batch_size, seq_len, emb_dim//2)\n",
    "\n",
    "        # Now we change x = self.trf_blocks(x) to: \n",
    "        for layer in self.trf_blocks:\n",
    "            x1, x2 = layer(x1, x2)\n",
    "        # merge\n",
    "        x = torch.cat([x1, x2], dim=-1)  # (b, s, dim)\n",
    "        \n",
    "        # Now as usual\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens: int = 250):\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:,-self.content_size:]\n",
    "            with torch.no_grad():\n",
    "                logits = self.forward(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probas = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a99631",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c15fa1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GPTModelRev(GPT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c62bcfaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModelRev(\n",
       "  (tok_emb): Embedding(50257, 256)\n",
       "  (pos_emb): Embedding(1024, 256)\n",
       "  (drop_emb): Dropout(p=0.05, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (1): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (2): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (3): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (4): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (5): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (6): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (7): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (8): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (9): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (10): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (11): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (12): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (13): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (14): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (15): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (16): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (17): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (18): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (19): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (20): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (21): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (22): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (23): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (24): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (25): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (26): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (27): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (28): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (29): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (30): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (31): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (norm2): RMSNorm((128,), eps=None, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm((256,), eps=None, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=256, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b56f09b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_data:\n",
    "    print(x.size())\n",
    "    r = m(x.to(device))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf1dedee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024, 50257])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657fd42",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4846802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7894f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, optimizer, params, device):\n",
    "        self.optimizer = optimizer\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "    \n",
    "    def train_model(self, \n",
    "                    model, \n",
    "                    tokenizer, \n",
    "                    train_dataloader, \n",
    "                    val_dataloader, \n",
    "                    writer: object = None, \n",
    "                    grad_accum: int = 1, \n",
    "                    max_norm: float = 1.0, \n",
    "                    scheduler: object = None, \n",
    "                    start_epoch: int = 0, \n",
    "                    path_to_save: str = ''):\n",
    "        #train_epoch_loss = []\n",
    "        val_loss = []\n",
    "        cumulative_tokens_get = []\n",
    "        tokens_get = 0\n",
    "        scaler = torch.amp.GradScaler(device=self.device)\n",
    "        t_d_len = len(train_dataloader)\n",
    "\n",
    "        for epoch in range(start_epoch, self.params['N_EPOCHS']):\n",
    "            total_loss = 0.0\n",
    "            self.optimizer.zero_grad()\n",
    "            for step, (x, y) in enumerate(train_dataloader):\n",
    "                if not (model.training):\n",
    "                    model.train()\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                tokens_get += len(x.flatten())\n",
    "                cumulative_tokens_get.append(tokens_get)\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    logits = model(x)\n",
    "                    loss = nn.functional.cross_entropy(logits.flatten(0, 1), y.flatten())\n",
    "\n",
    "                scaler.scale(loss).backward() #loss.backward()\n",
    "                curr_train_loss = loss.item()\n",
    "                total_loss += curr_train_loss\n",
    "                writer.add_scalar(\"Tokens get\", tokens_get, step)\n",
    "                writer.add_scalar(\"Current train loss\", curr_train_loss, step)\n",
    "\n",
    "                if (step % grad_accum == 0):\n",
    "                    scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        model.parameters(), max_norm\n",
    "                    )\n",
    "                    scaler.step(self.optimizer)\n",
    "                    scaler.update()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    if scheduler:\n",
    "                        scheduler.step()\n",
    "\n",
    "                if (self.params['verbose'] is True) and (step % self.params['sample_freq'] == 0):\n",
    "                    model.eval()\n",
    "                    sample = tokenizer.decode(model.generate(idx=torch.tensor(tokenizer('Я расскажу тебе про')['input_ids'], \n",
    "                                                                              device=device).unsqueeze(0), \n",
    "                                                                              max_new_tokens=15).squeeze(0).tolist())\n",
    "                    print(f'Epoch {epoch}, step {step} of {t_d_len}: Train loss = {loss}, sample: {sample}')\n",
    "\n",
    "                if (self.params['verbose'] is True) and (step % self.params['verbose_freq'] == 0) and (writer is not None):\n",
    "                    writer.add_scalar(\"Loss/train in step\", loss, epoch)\n",
    "                    writer.add_text(\"Sample\", str(sample), epoch)\n",
    "                    if (self.params['gradients'] is True):\n",
    "                        grads = []\n",
    "                        for name, param in model.named_parameters():\n",
    "                            if ('weight' in name):\n",
    "                                if (param.grad is not None):\n",
    "                                    grads.append(param.grad.abs().flatten().mean().cpu().detach().numpy())\n",
    "                        writer.add_scalar(\"train/gradients\", np.array(grads).flatten().mean(), epoch)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for x, y in val_dataloader:\n",
    "                            x, y = x.to(self.device), y.to(self.device)\n",
    "                            logits = model(x)\n",
    "                            loss = nn.functional.cross_entropy(logits.flatten(0, 1), y.flatten())\n",
    "                            val_loss.append(loss)\n",
    "                        if (writer is not None):\n",
    "                            writer.add_scalar(\"Loss/train in check\", torch.mean(torch.tensor(curr_train_loss, device='cpu')), epoch)\n",
    "                            writer.add_scalar(\"Loss/val in check\", torch.mean(torch.tensor(val_loss, device='cpu')), epoch)\n",
    "\n",
    "                if (step % self.params['save_checkpoint_freq'] == 0):\n",
    "                    # Сохранение чекпоинта (по желанию)\n",
    "                    #is_best = val_loss < best_loss\n",
    "                    #best_loss = min(val_loss, best_loss)\n",
    "                    checkpoint = {\n",
    "                        'epoch':           epoch,\n",
    "                        'model_state':     model.state_dict(),\n",
    "                        'optimizer_state': self.optimizer.state_dict(),\n",
    "                        'scheduler_state': scheduler.state_dict(),\n",
    "                    }\n",
    "                    torch.save(checkpoint, os.path.join(path_to_save, 'checkpoint_latest.pth'))\n",
    "                    #if is_best:\n",
    "                    #    torch.save(checkpoint, 'checkpoint_best.pth')\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3d15ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'N_EPOCHS': 1, \n",
    "          'verbose': True, \n",
    "          'verbose_freq': 140,\n",
    "          'save_checkpoint_freq': 140,\n",
    "          'sample_freq': 20,\n",
    "          'gradients': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2949c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModelRev(GPT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "202a0b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "375d92c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19452416"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amount_of_parameters = sum([p.numel() for p in model.parameters()])\n",
    "amount_of_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "243f91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(params=model.parameters(), lr=3e-4)\n",
    "grad_accum = 1\n",
    "warmup_ratio = 0.05  #0.1 # warm-up in first 10% of steps\n",
    "total_steps = (len(train_data) // grad_accum) * params[\"N_EPOCHS\"]\n",
    "#total_steps = params[\"N_EPOCHS\"] * len(train_data)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=total_steps)\n",
    "trainer = Trainer(optimizer=opt, params=params, device=device)\n",
    "\n",
    "# Scheduler: linear warm-up + cosine decay\n",
    "warmup_steps = int(warmup_ratio * total_steps)\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3064597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_model(model=model, \n",
    "                    tokenizer=tok, \n",
    "                    train_dataloader=train_data, \n",
    "                    val_dataloader=val_data, \n",
    "                    writer=writer, \n",
    "                    grad_accum=grad_accum, \n",
    "                    max_norm=1.0, \n",
    "                    scheduler=scheduler, \n",
    "                    start_epoch=0,\n",
    "                    path_to_save=CHECKPOINTS_PATH)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff7992b",
   "metadata": {},
   "source": [
    "Epoch 0, step 280 of 28228: Train loss = 5.100638389587402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf53e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.decode(generate(model=model, idx=torch.tensor(tok('Я хочу')['input_ids'], device='cuda').unsqueeze(0), max_new_tokens=15, context_size=1024).squeeze(0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b737e8",
   "metadata": {},
   "source": [
    "How to use tensorboard?  \n",
    "tensorboard --logdir=GPT_training or you name (instead of GPT_training) or tensorboard --logdir=runs  \n",
    "http://localhost:6006  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f46bde9",
   "metadata": {},
   "source": [
    "### Saving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e65d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"model.pth\") # without state of optimizer\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': opt.state_dict(),\n",
    "    }, \"model_and_optimizer.pth\") # with state of optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f69eb20",
   "metadata": {},
   "source": [
    "### Loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c1e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3635be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(0, 100, size=(10, 1024)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b77bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.randint(0, 100, size=(50, 1024))).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5aeb1",
   "metadata": {},
   "source": [
    "### Load model all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21836b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'N_EPOCHS': 1, \n",
    "          'verbose': True, \n",
    "          'verbose_freq': 140,\n",
    "          'save_checkpoint_freq': 140,\n",
    "          'sample_freq': 20,\n",
    "          'gradients': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7d51b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8908f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModelRev(GPT_CONFIG)\n",
    "model = model.to(device)\n",
    "loaded_model_checkpoint = torch.load(os.path.join(CHECKPOINTS_PATH, \"checkpoint_latest.pth\"))\n",
    "# checkpoint = {\n",
    "#                         'epoch':           epoch,\n",
    "#                         'model_state':     model.state_dict(),\n",
    "#                         'optimizer_state': self.optimizer.state_dict(),\n",
    "#                         'scheduler_state': scheduler.state_dict(),\n",
    "#                     }\n",
    "model.load_state_dict(loaded_model_checkpoint['model_state'])\n",
    "opt = torch.optim.AdamW(params=model.parameters(), lr=3e-4)\n",
    "opt.load_state_dict(loaded_model_checkpoint['optimizer_state'])\n",
    "grad_accum = 1\n",
    "warmup_ratio = 0.05  #0.1 # warm-up in first 10% of steps\n",
    "total_steps = (len(train_data) // grad_accum) * params[\"N_EPOCHS\"]\n",
    "# Scheduler: linear warm-up + cosine decay\n",
    "warmup_steps = int(warmup_ratio * total_steps)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "scheduler.load_state_dict(loaded_model_checkpoint['scheduler_state'])\n",
    "\n",
    "trainer = Trainer(optimizer=opt, params=params, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89e5134d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19452416"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amount_of_parameters = sum([p.numel() for p in model.parameters()])\n",
    "amount_of_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7be9ab32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28228"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53add4f2",
   "metadata": {},
   "source": [
    "### New training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f5365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, step 0 of 28228: Train loss = 2.8201711177825928, sample: Я расскажу тебе про при при пораз\n"
     ]
    }
   ],
   "source": [
    "trainer.train_model(model=model, \n",
    "                    tokenizer=tok, \n",
    "                    train_dataloader=train_data, \n",
    "                    val_dataloader=val_data, \n",
    "                    writer=writer, \n",
    "                    grad_accum=grad_accum, \n",
    "                    max_norm=1.0, \n",
    "                    scheduler=scheduler, \n",
    "                    start_epoch=loaded_model_checkpoint['epoch'],\n",
    "                    path_to_save=CHECKPOINTS_PATH)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a4ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6c78c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d918ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f389ceea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
