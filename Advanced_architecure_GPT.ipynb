{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9ad5b2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "479f4b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint, checkpoint_sequential\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e52c0",
   "metadata": {},
   "source": [
    "Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96cbe8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getenv(\"PATH\")\n",
    "DATAPATH = os.getenv(\"DATAPATH\")\n",
    "PREPARED_DATA_DIR = os.getenv(\"PREPARED_DATA_DIR\")\n",
    "CACHE_DIR = os.getenv(\"CACHE_DIR\")\n",
    "#TOK_NAME = \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\"\n",
    "TOK_NAME = os.getenv(\"TOK_NAME\")\n",
    "PARQUET_DATA_DIR = os.getenv(\"PARQUET_DATA_DIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d437f51",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9db08731",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "    'vocab_size': 50257, # in 151670 (if you use tokenizer.vocab_size then you get partial vocab_size without added tokens)\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 256, #768\n",
    "    'n_heads': 4,#12,\n",
    "    'n_layers': 4,#12,\n",
    "    'drop_rate': 0.05, # 0l1\n",
    "    'qkv_bias': False,\n",
    "    'num_segments': 2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98fcc0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if (torch.cuda.is_available()) else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbe7364",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee0e4a",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e222b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = transformers.AutoTokenizer.from_pretrained(TOK_NAME, cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b58c7c7",
   "metadata": {},
   "source": [
    "Check tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "febbce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tok.get_added_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5edd7623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad58e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If tokenizer dont have pad_token\n",
    "tok.pad_token = tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "394abeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok('Привет, как дела mhjm', return_tensors='pt', padding='max_length', max_length=2048)['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82898a5c",
   "metadata": {},
   "source": [
    "## Actual version of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3cdc0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetV3(Dataset):\n",
    "    def __init__(self, dataframe: str, tokenizer: object, max_length: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        for i, curr_chunk in tqdm(dataframe.iterrows(), total=dataframe.shape[0]):\n",
    "            token_ids = tokenizer(curr_chunk['Sample'], return_tensors='pt', padding='max_length', max_length=max_length+1)['input_ids']\n",
    "            input_chunk = token_ids[:,:max_length].view(-1)\n",
    "            target_chunk = token_ids[:,1:max_length+1].view(-1)\n",
    "            #print(input_chunk.size(), target_chunk.size(),)\n",
    "            self.input_ids.append(input_chunk)\n",
    "            self.target_ids.append(target_chunk)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8af056",
   "metadata": {},
   "source": [
    "# Load actual data and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76cccaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = pd.read_parquet(PARQUET_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5480f9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224662, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_parquet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "989757da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77972913831b40bea11b8b4cbc485251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_cd = CustomDatasetV3(dataframe=data_parquet.iloc[:100000], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])\n",
    "train_cd = CustomDatasetV3(dataframe=data_parquet.iloc[:100], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2de0dbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900fc71e98864a209f865ea822d37903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#val_cd = CustomDatasetV3(dataframe=data_parquet.iloc[-10000:], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])\n",
    "val_cd = CustomDatasetV3(dataframe=data_parquet.iloc[-100:], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb02d6",
   "metadata": {},
   "source": [
    "batch_size maybe 8 or 12 (or 16) check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96022e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(dataset=train_cd, batch_size=4, shuffle=True, num_workers=0) # num_workers=2 don't work?\n",
    "val_data = DataLoader(dataset=val_cd, batch_size=4, shuffle=True, num_workers=0) # num_workers=2 don't work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "081aeea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[12466,   251, 16142,  ..., 12466,   123, 25443],\n",
       "         [12466,   240,   220,  ..., 16843, 20375, 21727],\n",
       "         [12466,   248, 21169,  ...,   114, 18849,   140],\n",
       "         [12466,   253, 15166,  ..., 20375, 45367, 12466]]),\n",
       " tensor([[  251, 16142, 12466,  ...,   123, 25443,   112],\n",
       "         [  240,   220, 21727,  ..., 20375, 21727, 20375],\n",
       "         [  248, 21169, 18849,  ..., 18849,   140,   115],\n",
       "         [  253, 15166, 12466,  ..., 45367, 12466,   122]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18d2ad0",
   "metadata": {},
   "source": [
    "# LLM Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39c86f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionDP_QKV(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0)\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = dropout#nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.size()\n",
    "        qkv = self.W_qkv(x) # b, num_tokens, 3 * self.d_out\n",
    "        queries, keys, values = qkv.split(self.d_out, dim=2)\n",
    "\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        queries = queries.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "        keys = keys.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "        values = values.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "\n",
    "        # All code below we replace with torch.nn.functional.scaled_dot_product_attention\n",
    "        context_vec = torch.nn.functional.scaled_dot_product_attention(queries, keys, values, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "\n",
    "        # att_scores = queries @ keys.transpose(2, 3) # shapes = (num_tokens, self.head_dim) @ (self.head_dim, num_tokens) -> (num_tokens, num_tokens)\n",
    "        # mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        # att_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        # att_weights = torch.softmax(att_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        # att_weights = self.dropout(att_weights)\n",
    "        # context_vec = (att_weights @ values).transpose(1, 2) # (num_tokens, num_tokens) @ (num_tokens, self.head_dim) -> (num_tokens, self.head_dim) -> transpose(1,2) of (b, self.num_heads, num_tokens, self.head_dim) ->\n",
    "        # # -> (b, num_tokens, self.num_heads, self.head_dim) as view in previous code after inference of Linear layers\n",
    "        \n",
    "        # Reshape etc\n",
    "        context_vec = context_vec.transpose(1, 2).contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8026aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mha_dp_qkv = MultiHeadAttentionDP_QKV(d_in=embed_dim, d_out=embed_dim, context_length=context_len, dropout=dropout, num_heads=num_heads, qkv_bias=qkv_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0973dd6a",
   "metadata": {},
   "source": [
    "## Additional classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fbc2483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim),\n",
    "            nn.Mish(), #GELU(),\n",
    "            nn.Linear(4 * emb_dim, emb_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1be1a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversibleTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        emb_dim = cfg['emb_dim']//2\n",
    "        self.attn = MultiHeadAttentionDP_QKV(d_in=emb_dim, \n",
    "                                       d_out=emb_dim, \n",
    "                                       context_length=cfg['context_length'], \n",
    "                                       dropout=cfg['drop_rate'], \n",
    "                                       num_heads=cfg['n_heads'], \n",
    "                                       qkv_bias=cfg['qkv_bias'])\n",
    "        self.ff = FeedForward(emb_dim)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim) #LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = nn.LayerNorm(emb_dim) #LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_resid = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # reversible update\n",
    "        # y1 = x1 + f(x2)\n",
    "        # y2 = x2 + g(y1)\n",
    "\n",
    "        def f(u):\n",
    "            u = self.norm1(u)\n",
    "            attn_output = self.attn(u)\n",
    "            attn_output = self.drop_resid(attn_output)\n",
    "            return attn_output\n",
    "        \n",
    "        def g(v):\n",
    "            return self.drop_resid(self.ff(self.norm2(v)))\n",
    "        \n",
    "        f_x2 = checkpoint(f, x2)\n",
    "        y1 = x1 + f_x2\n",
    "        g_y1 = checkpoint(g, y1)\n",
    "        y2 = x2 + g_y1\n",
    "\n",
    "        return y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71018848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModelRev(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[ReversibleTransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "        self.final_norm = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.size()\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # initialize reversible pairs: split features\n",
    "        # split last dim\n",
    "        x1, x2 = torch.chunk(x, 2, dim=-1)  # each (batch_size, seq_len, emb_dim//2)\n",
    "\n",
    "        # Now we change x = self.trf_blocks(x) to: \n",
    "        for layer in self.trf_blocks:\n",
    "            x1, x2 = layer(x1, x2)\n",
    "        # merge\n",
    "        x = torch.cat([x1, x2], dim=-1)  # (b, s, dim)\n",
    "        \n",
    "        # Now as usual\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a502431e",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca1de86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a99631",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c15fa1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GPTModelRev(GPT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b56f09b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x:\\Users\\jeka_\\miniconda3\\envs\\LLM\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_data:\n",
    "    print(x.size())\n",
    "    r = m(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf1dedee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1024, 50257])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657fd42",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4846802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7894f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, optimizer, params, device):\n",
    "        self.optimizer = optimizer\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "    \n",
    "    def train_model(self, model, tokenizer, train_dataloader, val_dataloader, writer=None, grad_accum=1, max_norm=1.0, scheduler=None):\n",
    "        train_epoch_loss = []\n",
    "        val_loss = []\n",
    "        cumulative_tokens_get = []\n",
    "        tokens_get = 0\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        for epoch in range(self.params['N_EPOCHS']):\n",
    "            total_loss = 0.0\n",
    "            self.optimizer.zero_grad()\n",
    "            for step, (x, y) in enumerate(train_dataloader):\n",
    "                if not (model.training):\n",
    "                    model.train()\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                tokens_get += len(x.flatten())\n",
    "                cumulative_tokens_get.append(tokens_get)\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    logits = model(x)\n",
    "                    loss = nn.functional.cross_entropy(logits.flatten(0, 1), y.flatten())\n",
    "\n",
    "                scaler.scale(loss).backward() #loss.backward()\n",
    "                curr_train_loss = loss.item()\n",
    "                total_loss += curr_train_loss\n",
    "                writer.add_scalar(\"Tokens get\", tokens_get, step)\n",
    "\n",
    "                if (step % grad_accum == 0):\n",
    "                    scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        model.parameters(), max_norm\n",
    "                    )\n",
    "                    scaler.step(self.optimizer)\n",
    "                    scaler.update()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    if scheduler:\n",
    "                        scheduler.step()\n",
    "\n",
    "                if (self.params['verbose'] is True) and (tokens_get % self.params['verbose_freq'] == 0):\n",
    "                    sample = tokenizer.decode(generate(model=model, idx=torch.tensor(tokenizer('Я большая языковая модель и ')['input_ids'], device=self.device).unsqueeze(0), max_new_tokens=25, context_size=1024).squeeze(0).tolist())\n",
    "                    print(f'Epoch {epoch}: Train loss = {loss}, sample: {sample}')\n",
    "                    if (writer is not None):\n",
    "                        writer.add_scalar(\"Loss/train in step\", loss, epoch)\n",
    "                        writer.add_text(\"Sample\", str(sample), epoch)\n",
    "                        if (self.params['gradients'] is True):\n",
    "                            grads = []\n",
    "                            for name, param in model.named_parameters():\n",
    "                                if ('weight' in name):\n",
    "                                    if (param.grad is not None):\n",
    "                                        grads.append(param.grad.abs().flatten().mean().cpu().detach().numpy())\n",
    "                            writer.add_scalar(\"train/gradients\", np.array(grads).flatten().mean(), epoch)\n",
    "\n",
    "\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        for x, y in val_dataloader:\n",
    "                            x, y = x.to(self.device), y.to(self.device)\n",
    "                            logits = model(x)\n",
    "                            loss = nn.functional.cross_entropy(logits.flatten(0, 1), y.flatten())\n",
    "                            val_loss.append(loss)\n",
    "                        if (writer is not None):\n",
    "                            writer.add_scalar(\"Loss/train in check\", torch.mean(torch.tensor(curr_train_loss, device='cpu')), epoch)\n",
    "                            writer.add_scalar(\"Loss/val in check\", torch.mean(torch.tensor(val_loss, device='cpu')), epoch)\n",
    "            writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3d15ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'N_EPOCHS': 5, \n",
    "          'verbose': True, \n",
    "          'verbose_freq': 1,\n",
    "          'gradients': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2949c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModelRev(GPT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "202a0b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed044a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModelRev(\n",
       "  (tok_emb): Embedding(50257, 256)\n",
       "  (pos_emb): Embedding(1024, 256)\n",
       "  (drop_emb): Dropout(p=0.05, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (1): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (2): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (3): ReversibleTransformerBlock(\n",
       "      (attn): MultiHeadAttentionDP_QKV(\n",
       "        (W_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): Mish()\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_resid): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=256, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "375d92c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26785792"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amount_of_parameters = sum([p.numel() for p in model.parameters()])\n",
    "amount_of_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1272eadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243f91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(params=model.parameters(), lr=0.01)\n",
    "grad_accum = 2\n",
    "total_steps = (len(train_data) // grad_accum) * params[\"N_EPOCHS\"]\n",
    "#total_steps = params[\"N_EPOCHS\"] * len(train_data)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=total_steps\n",
    "trainer = Trainer(optimizer=opt, params=params, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3064597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeka_\\AppData\\Local\\Temp\\ipykernel_148776\\2616554926.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "x:\\Users\\jeka_\\miniconda3\\envs\\LLM\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "x:\\Users\\jeka_\\miniconda3\\envs\\LLM\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\jeka_\\AppData\\Local\\Temp\\ipykernel_148776\\2616554926.py:55: RuntimeWarning: Mean of empty slice.\n",
      "  writer.add_scalar(\"train/gradients\", np.array(grads).flatten().mean(), epoch)\n",
      "x:\\Users\\jeka_\\miniconda3\\envs\\LLM\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss = 5.437976837158203, sample: Я большая языковая модель и  �о � � �о �о � �о � � � �о �о � �о � �о �\n",
      "Epoch 0: Train loss = 3.93634033203125, sample: Я большая языковая модель и  �о � � �о �о � �о � � � �о �о � �о � �о �\n",
      "Epoch 0: Train loss = 3.9625244140625, sample: Я большая языковая модель и  �о � � �о �о � �о � � � �о �о � �о � �о �\n",
      "Epoch 0: Train loss = 3.9942569732666016, sample: Я большая языковая модель и  �о � � �о �о � �о � � � �о �о � �о � �о �\n",
      "Epoch 0: Train loss = 4.015851974487305, sample: Я большая языковая модель и ееееееееееееее�ееееееееее\n",
      "Epoch 0: Train loss = 3.8237686157226562, sample: Я большая языковая модель и ееееееееееееее�ееееееееее\n",
      "Epoch 0: Train loss = 3.8851795196533203, sample: Я большая языковая модель и ееееееееееееее�ееееееееее\n",
      "Epoch 0: Train loss = 3.9865798950195312, sample: Я большая языковая модель и ееееееееееееее�ееееееееее\n",
      "Epoch 0: Train loss = 3.8604507446289062, sample: Я большая языковая модель и со�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�\n",
      "Epoch 0: Train loss = 4.037256240844727, sample: Я большая языковая модель и со�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�\n",
      "Epoch 0: Train loss = 4.251428604125977, sample: Я большая языковая модель и со�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�\n",
      "Epoch 0: Train loss = 4.033658981323242, sample: Я большая языковая модель и со�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�о�\n",
      "Epoch 0: Train loss = 4.37578010559082, sample: Я большая языковая модель и стистистиистиститиститист\n",
      "Epoch 0: Train loss = 3.6254043579101562, sample: Я большая языковая модель и стистистиистиститиститист\n",
      "Epoch 0: Train loss = 3.738187789916992, sample: Я большая языковая модель и стистистиистиститиститист\n",
      "Epoch 0: Train loss = 3.8040733337402344, sample: Я большая языковая модель и стистистиистиститиститист\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_accum\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m writer.flush()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mTrainer.train_model\u001b[39m\u001b[34m(self, model, tokenizer, train_dataloader, val_dataloader, writer, grad_accum, max_norm, scheduler)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m val_dataloader:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m         x, y = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     62\u001b[39m         logits = model(x)\n\u001b[32m     63\u001b[39m         loss = nn.functional.cross_entropy(logits.flatten(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m), y.flatten())\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train_model(model=model, \n",
    "                    tokenizer=tok, \n",
    "                    train_dataloader=train_data, \n",
    "                    val_dataloader=val_data, \n",
    "                    writer=writer, \n",
    "                    grad_accum=grad_accum, \n",
    "                    scheduler=scheduler)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b737e8",
   "metadata": {},
   "source": [
    "How to use tensorboard?  \n",
    "tensorboard --logdir=GPT_training or you name (instead of GPT_training) or tensorboard --logdir=runs  \n",
    "http://localhost:6006  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f46bde9",
   "metadata": {},
   "source": [
    "### Saving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e65d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"model.pth\") # without state of optimizer\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': opt.state_dict(),\n",
    "    }, \"model_and_optimizer.pth\") # with state of optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f69eb20",
   "metadata": {},
   "source": [
    "### Loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c1e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3635be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(0, 100, size=(10, 1024)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b77bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.randint(0, 100, size=(50, 1024))).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5aeb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d918ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f389ceea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
