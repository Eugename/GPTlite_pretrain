{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c726bcc",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8298f5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b1e1b",
   "metadata": {},
   "source": [
    "Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23b4cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getenv(\"PATH\")\n",
    "DATAPATH = os.getenv(\"DATAPATH\")\n",
    "PREPARED_DATA_DIR = os.getenv(\"PREPARED_DATA_DIR\")\n",
    "CACHE_DIR = os.getenv(\"CACHE_DIR\")\n",
    "#TOK_NAME = \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\"\n",
    "TOK_NAME = os.getenv(\"TOK_NAME\")\n",
    "PARQUET_DATA_DIR = os.getenv(\"PARQUET_DATA_DIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b775b8ad",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d87686",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = {\n",
    "    'vocab_size': 50257, # in 151670 (if you use tokenizer.vocab_size then you get partial vocab_size without added tokens)\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 768, #768\n",
    "    'n_heads': 2,#12,\n",
    "    'n_layers': 2,#12,\n",
    "    'drop_rate': 0.05, # 0l1\n",
    "    'qkv_bias': False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35772b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if (torch.cuda.is_available()) else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8d333",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d27ee4",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b37c1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = transformers.AutoTokenizer.from_pretrained(TOK_NAME, cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58342819",
   "metadata": {},
   "source": [
    "Check tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ea59c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.get_added_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c577f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c778b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If tokenizer dont have pad_token\n",
    "tok.pad_token = tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e84b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok('Привет, как дела mhjm', return_tensors='pt', padding='max_length', max_length=2048)['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a3216",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4621685d",
   "metadata": {},
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b5928",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATAPATH, encoding='utf8', mode='r') as file:\n",
    "    d = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1423181938//131072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a681986",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chunks=25\n",
    "stride = len(d)//num_chunks\n",
    "\n",
    "for i, chunk_idx in tqdm(enumerate(range(0, len(d), stride))):\n",
    "    with open(os.path.join(PREPARED_DATA_DIR, f'chunk_{i}.txt'), mode='w') as file:\n",
    "        file.write(d[chunk_idx:chunk_idx+stride])\n",
    "    print(i, chunk_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0080193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344858be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_chunks=25\n",
    "stride = 131072#len(d)//num_chunks\n",
    "\n",
    "data_parquet = pd.DataFrame([], columns=['Sample', 'Chunk'])\n",
    "for i, chunk_idx in tqdm(enumerate(range(0, len(d), stride))):\n",
    "    data_parquet.loc[len(data_parquet)] = ['sdgsgsg', 0]\n",
    "    #with open(os.path.join(PREPARED_DATA_DIR, f'chunk_{i}.txt'), mode='w') as file:\n",
    "    #    file.write(d[chunk_idx:chunk_idx+stride])\n",
    "    print(i, chunk_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9db9b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = pd.DataFrame([], columns=['Sample', 'Chunk'])\n",
    "for i, filename in tqdm(enumerate(os.listdir(PREPARED_DATA_DIR)), total=len(os.listdir(PREPARED_DATA_DIR))):\n",
    "    with open(os.path.join(PREPARED_DATA_DIR, filename), encoding='utf8', mode='r') as file:\n",
    "        current_file = file.read()\n",
    "        stride = 2048*3\n",
    "        mas = ''\n",
    "        for article in current_file.split('/n'):\n",
    "            for sentence in article.split('.'):\n",
    "                if (len(mas)+len(sentence) < stride):\n",
    "                    mas += sentence\n",
    "                else:\n",
    "                    data_parquet.loc[len(data_parquet)] = [mas, i]\n",
    "                    mas = ''\n",
    "            \n",
    "        # for chunk_idx in tqdm(range(0, len(current_file), stride)):\n",
    "        #     current_chunk = current_file[chunk_idx:chunk_idx+stride]\n",
    "        #     data_parquet.loc[len(data_parquet)] = ['sdgsgsg', 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a51343",
   "metadata": {},
   "outputs": [],
   "source": [
    "2048*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet.to_parquet(PARQUET_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "d[200:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cad838d",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ae8b4",
   "metadata": {},
   "source": [
    "Небольшой анализ длины предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PREPARED_DATA_DIR, os.listdir(PREPARED_DATA_DIR)[0]), encoding='utf8', mode='r') as file:\n",
    "    d = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "splt = d.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(elem) for elem in splt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a3c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ec951",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lens, bins=20, range=(0, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5257816f",
   "metadata": {},
   "source": [
    "Если взять длину абзацев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de5f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "splt = d.split('/n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e88d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splt[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75531036",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b3437",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(elem) for elem in splt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9451653",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lens, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9592ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1155cac5",
   "metadata": {},
   "source": [
    "Class for dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9daaf8c",
   "metadata": {},
   "source": [
    "## Old versions of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c3040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetV1(Dataset):\n",
    "    def __init__(self, txt: str, tokenizer: object, max_length: int, stride: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd7efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetV2(Dataset):\n",
    "    def __init__(self, dataframe: str, tokenizer: object, max_length: int, stride: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        for i, curr_chunk in dataframe.iterrows():\n",
    "            token_ids = tokenizer.encode(curr_chunk['Sample'])\n",
    "            for i in range(0, len(token_ids) - max_length, stride):\n",
    "                input_chunk = token_ids[i:i + max_length]\n",
    "                target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "                self.input_ids.append(torch.tensor(input_chunk))\n",
    "                self.target_ids.append(torch.tensor(target_chunk))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ebfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = CustomDatasetV2(dataframe=data_parquet.iloc[:100], tokenizer=tok, max_length=1024, stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2fa17e",
   "metadata": {},
   "source": [
    "## Actual version of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4acec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetV3(Dataset):\n",
    "    def __init__(self, dataframe: str, tokenizer: object, max_length: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        for i, curr_chunk in tqdm(dataframe.iterrows(), total=dataframe.shape[0]):\n",
    "            token_ids = tokenizer(curr_chunk['Sample'], return_tensors='pt', padding='max_length', max_length=max_length+1)['input_ids']\n",
    "            input_chunk = token_ids[:,:max_length].view(-1)\n",
    "            target_chunk = token_ids[:,1:max_length+1].view(-1)\n",
    "            #print(input_chunk.size(), target_chunk.size(),)\n",
    "            self.input_ids.append(input_chunk)\n",
    "            self.target_ids.append(target_chunk)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7457f136",
   "metadata": {},
   "source": [
    "# Load actual data and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3528bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = pd.read_parquet(PARQUET_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c582dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121589e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cd = CustomDatasetV3(dataframe=data_parquet.iloc[:100000], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])\n",
    "#train_cd = CustomDatasetV3(dataframe=data_parquet.iloc[:100], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b00637",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cd = CustomDatasetV3(dataframe=data_parquet.iloc[-10000:], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#MY_GPT_CONFIG['context_length'])\n",
    "#val_cd = CustomDatasetV3(dataframe=data_parquet.iloc[-100:], tokenizer=tok, max_length=GPT_CONFIG['context_length'])#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(dataset=train_cd, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_data = DataLoader(dataset=val_cd, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a74efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fce445",
   "metadata": {},
   "source": [
    "# LLM Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b42e13f",
   "metadata": {},
   "source": [
    "## Simple Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e3b06",
   "metadata": {},
   "source": [
    "### With prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9f43119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0)\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.size()\n",
    "        keys = self.W_key(x) # b, num_tokens, self.d_out\n",
    "        queries = self.W_query(x) # b, num_tokens, self.d_out\n",
    "        values = self.W_value(x) # b, num_tokens, self.d_out\n",
    "        print('values.shape is ' , values.shape)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        print('values.shape (after view) is ' , values.shape)\n",
    "\n",
    "        keys = keys.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "        queries = queries.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "        values = values.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "        print('values.shape (after transpose) is ' , values.shape)\n",
    "\n",
    "        att_scores = queries @ keys.transpose(2, 3) # shapes = (num_tokens, self.head_dim) @ (self.head_dim, num_tokens) -> (num_tokens, num_tokens)\n",
    "        print('att_scores.shape is ' , att_scores.shape)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        att_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        att_weights = torch.softmax(att_scores / keys.shape[-1]**.5, dim=-1)\n",
    "        att_weights = self.dropout(att_weights)\n",
    "\n",
    "        context_vec = (att_weights @ values).transpose(1, 2) # (num_tokens, num_tokens) @ (num_tokens, self.head_dim) -> (num_tokens, self.head_dim) -> transpose(1,2) of (b, self.num_heads, num_tokens, self.head_dim) ->\n",
    "        # -> (b, num_tokens, self.num_heads, self.head_dim) as view in previous code after inference of Linear layers\n",
    "        print('context_vec.shape is ' , att_scores.shape)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        print('context_vec.shape is ' , att_scores.shape)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24785a31",
   "metadata": {},
   "source": [
    "#### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d15f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(d_in=768, d_out=768, context_length=2048, dropout=0.1, num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d01cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(10, 2048, 768).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001867d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mha(torch.rand(10, 2048, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91e6b9b",
   "metadata": {},
   "source": [
    "### Without prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9babe64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0)\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.size()\n",
    "        keys = self.W_key(x) # b, num_tokens, self.d_out\n",
    "        queries = self.W_query(x) # b, num_tokens, self.d_out\n",
    "        values = self.W_value(x) # b, num_tokens, self.d_out\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "        queries = queries.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "        values = values.transpose(1, 2) # b, self.num_heads, num_tokens, self.head_dim\n",
    "\n",
    "        att_scores = queries @ keys.transpose(2, 3) # shapes = (num_tokens, self.head_dim) @ (self.head_dim, num_tokens) -> (num_tokens, num_tokens)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        att_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        att_weights = torch.softmax(att_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        att_weights = self.dropout(att_weights)\n",
    "\n",
    "        context_vec = (att_weights @ values).transpose(1, 2) # (num_tokens, num_tokens) @ (num_tokens, self.head_dim) -> (num_tokens, self.head_dim) -> transpose(1,2) of (b, self.num_heads, num_tokens, self.head_dim) ->\n",
    "        # -> (b, num_tokens, self.num_heads, self.head_dim) as view in previous code after inference of Linear layers\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b4aadd",
   "metadata": {},
   "source": [
    "## Other Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads,):\n",
    "        super().__init__()\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(d_in, 3 * d_in)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(d_in, d_in)\n",
    "        # regularization\n",
    "        self.n_head = num_heads\n",
    "        self.n_embd = d_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6cb10",
   "metadata": {},
   "source": [
    "## Additional classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a146d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f7d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LayerNorm(nn.Module):\n",
    "#     def __init__(self, emb_dim):\n",
    "#         super().__init__()\n",
    "#         self.eps = 1e-5\n",
    "#         self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "#         self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean = x.mean(dim=-1, keepdim=True)\n",
    "#         var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "#         norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "#         return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GELU(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return 0.5 * x * (1 + torch.tanh( torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1ba07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa002c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim'])\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca274e7",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20f8f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_in=cfg['emb_dim'], \n",
    "                                       d_out=cfg['emb_dim'], \n",
    "                                       context_length=cfg['context_length'], \n",
    "                                       dropout=cfg['drop_rate'], \n",
    "                                       num_heads=cfg['n_heads'], \n",
    "                                       qkv_bias=cfg['qkv_bias'])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_resid = nn.Dropout(cfg['drop_rate'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = x + self.drop_resid(self.attn(self.norm1(x)))\n",
    "        #x = x + self.drop_resid(self.ff(self.norm2(x)))\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a88ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerBlock(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.att = MultiHeadAttention(\n",
    "#             d_in=cfg[\"emb_dim\"],\n",
    "#             d_out=cfg[\"emb_dim\"],\n",
    "#             context_length=cfg[\"context_length\"],\n",
    "#             num_heads=cfg[\"n_heads\"], \n",
    "#             dropout=cfg[\"drop_rate\"],\n",
    "#             qkv_bias=cfg[\"qkv_bias\"])\n",
    "#         self.ff = FeedForward(cfg)\n",
    "#         self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "#         self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "#         self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Shortcut connection for attention block\n",
    "#         shortcut = x\n",
    "#         x = self.norm1(x)\n",
    "#         x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "#         x = self.drop_shortcut(x)\n",
    "#         x = x + shortcut  # Add the original input back\n",
    "\n",
    "#         # Shortcut connection for feed forward block\n",
    "#         shortcut = x\n",
    "#         x = self.norm2(x)\n",
    "#         x = self.ff(x)\n",
    "#         x = self.drop_shortcut(x)\n",
    "#         x = x + shortcut  # Add the original input back\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03704a61",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6059426",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TransformerBlock(GPT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99239cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_size = 10, 1024, 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ccd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb(torch.rand(*init_size, device='cpu')).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e95bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb(torch.rand(*init_size, device='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f041287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = nn.Sequential(*[TransformerBlock(GPT_CONFIG) for _ in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7fa932",
   "metadata": {},
   "outputs": [],
   "source": [
    "s(torch.rand(*init_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s(torch.rand(*init_size)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73056cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_size == s(torch.rand(*init_size)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b625ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_size == tb(torch.rand(init_size)).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875a52ee",
   "metadata": {},
   "source": [
    "## GPT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e9cc587",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "        self.final_norm = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.size()\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8cbcd",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc43b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GPTModel(GPT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in data_load:\n",
    "    print(x.size())\n",
    "    r = m(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b37704",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab15640c",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d243e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8510dff",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d74e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(tok('Привет, как дела ')['input_ids']).unsqueeze(0), #tok('Привет, как дела mhjm', return_tensors='pt', padding='max_length', max_length=2048)['input_ids'].shape\n",
    "# tok('Привет, как дела mhjm', return_tensors='pt', padding='max_length', max_length=1024)['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c691c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.decode(generate(model=m, idx=torch.tensor(tok('Привет, как дела ')['input_ids']).unsqueeze(0), max_new_tokens=20, context_size=1024).squeeze(0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a03f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1,2,3]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63039892",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1,2,3]).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31dc85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[1,2,3]]).squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c7569",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1653599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c25df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, optimizer, params, device):\n",
    "        self.optimizer = optimizer\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "    \n",
    "    def train_model(self, model, tokenizer, train_dataloader, val_dataloader, writer=None):\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        tokens_get = 0\n",
    "        for epoch in range(self.params['N_EPOCHS']):\n",
    "\n",
    "            \n",
    "            for x, y in train_dataloader:\n",
    "                if not (model.training):\n",
    "                    model.train()\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = model(x)\n",
    "                loss = nn.functional.cross_entropy(logits.flatten(0, 1), y.flatten())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss.append(loss)\n",
    "                tokens_get += len(x.flatten())\n",
    "\n",
    "                if (self.params['verbose'] is True) and (tokens_get % self.params['verbose_freq'] == 0):\n",
    "                    sample = tokenizer.decode(generate(model=model, idx=torch.tensor(tokenizer('Я большая языковая модель и ')['input_ids'], device=self.device).unsqueeze(0), max_new_tokens=25, context_size=1024).squeeze(0).tolist())\n",
    "                    print(f'Epoch {epoch}: Train loss = {loss}, sample: {sample}')\n",
    "                    if (writer is not None):\n",
    "                        writer.add_scalar(\"Loss/train in step\", loss, epoch)\n",
    "                        writer.add_text(\"Sample\", str(sample), epoch)\n",
    "                        if (self.params['gradients'] is True):\n",
    "                            grads = []\n",
    "                            for name, param in model.named_parameters():\n",
    "                                if ('weight' in name):\n",
    "                                    grads.append(param.grad.abs().flatten().mean().cpu().detach().numpy())\n",
    "                            writer.add_scalar(\"train/gradients\", np.array(grads).flatten().mean(), epoch)\n",
    "\n",
    "\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        for x, y in val_dataloader:\n",
    "                            x, y = x.to(self.device), y.to(self.device)\n",
    "                            logits = model(x)\n",
    "                            loss = nn.functional.cross_entropy(logits.flatten(0, 1), y.flatten())\n",
    "                            val_loss.append(loss)\n",
    "                        if (writer is not None):\n",
    "                            writer.add_scalar(\"Loss/train in check\", torch.mean(torch.tensor(train_loss, device='cpu')), epoch)\n",
    "                            writer.add_scalar(\"Loss/val in check\", torch.mean(torch.tensor(val_loss, device='cpu')), epoch)\n",
    "            writer.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'N_EPOCHS': 5, \n",
    "          'verbose': True, \n",
    "          'verbose_freq': 1,\n",
    "          'gradients': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96064d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50042235",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36936ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(params=model.parameters(), lr=0.01)\n",
    "trainer = Trainer(optimizer=opt, params=params, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_model(model=model, tokenizer=tok, train_dataloader=train_data, val_dataloader=val_data, writer=writer)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b8e6e1",
   "metadata": {},
   "source": [
    "How to use tensorboard?  \n",
    "tensorboard --logdir=GPT_training or you name (instead of GPT_training) or tensorboard --logdir=runs  \n",
    "http://localhost:6006  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8a0d5",
   "metadata": {},
   "source": [
    "### Saving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5693566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"model.pth\") # without state of optimizer\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': opt.state_dict(),\n",
    "    }, \"model_and_optimizer.pth\") # with state of optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a67bb",
   "metadata": {},
   "source": [
    "### Loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411cdd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(0, 100, size=(10, 1024)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.randint(0, 100, size=(50, 1024))).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3eb822",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e049454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
